{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHSY3xwlKZ2m"
      },
      "source": [
        "TF Pruning\n",
        "\n",
        "\n",
        "*   Position code\n",
        "*   Twacs\n",
        "*   DownStrem\n",
        "*   Phase Ground Down\n",
        "*   Not DownStrem\n",
        "*   Phase Ground Not Down\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGjfDi3TVtD0"
      },
      "outputs": [],
      "source": [
        "# sample\n",
        "# import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# ext_layer = -6\n",
        "# ext_phase_nt_down = '/content/class_code_202006_202206_all_classes_not_down_25k_best_InceptionTimeV2_phase_not_down_transfer.h5'\n",
        "# ext_phase_not_down =  keras.models.load_model(ext_phase_nt_down)\n",
        "# phase_not_down_model = keras.Model(inputs=ext_phase_not_down.inputs,outputs= ext_phase_not_down.layers[ext_layer].output)\n",
        "# phase_not_down_model.save('phase_not_down_ext.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcQlIqeiL6Xa"
      },
      "source": [
        "## For deployment file creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qmFplBklMcwv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1- Prune the extractors"
      ],
      "metadata": {
        "id": "UsM9C7E7k4t1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRdIcr1ePb4e"
      },
      "source": [
        "POS pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuTE_Vu8L5SJ",
        "outputId": "97081e11-d5d0-4e76-8fa4-5347e29b91c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "ext_layer = -2\n",
        "ext_pos_base = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_InceptionTime_pos_code_base.h5'\n",
        "ext_pos_base_reduced_layer =  keras.models.load_model(ext_pos_base)\n",
        "reduced_pos_model = keras.Model(inputs=ext_pos_base_reduced_layer.inputs,outputs= ext_pos_base_reduced_layer.layers[ext_layer].output)\n",
        "reduced_pos_model.save('/content/drive/MyDrive/VERSION5/pruned_models/pos_code_base_pruned_2layers.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KanfxcoPRjOn"
      },
      "source": [
        "Twacs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmJ4sisTLq0G",
        "outputId": "018f580f-db6d-4033-835f-658da744c662"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "ext_layer = -2\n",
        "ext_twacs_base = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_InceptionTime_TWACS_base.h5'\n",
        "ext_twacs_base_reduced_layer =  keras.models.load_model(ext_twacs_base)\n",
        "reduced_twacs_model = keras.Model(inputs=ext_twacs_base_reduced_layer.inputs,outputs= ext_twacs_base_reduced_layer.layers[ext_layer].output)\n",
        "reduced_twacs_model.save('/content/drive/MyDrive/VERSION5/pruned_models/twacs_code_base_pruned_2layers.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYnl9dyjTQfM"
      },
      "source": [
        "Down stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2i6l67EWM4M",
        "outputId": "c130da05-6cb1-4bb8-a152-c46d911bd36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "ext_layer = -2\n",
        "ext_down_base = '/content/drive/MyDrive/VERSION5/LSTM_models/best_InceptionTime_down_base_VERSION5.h5'\n",
        "ext_down_base_reduced_layer =  keras.models.load_model(ext_down_base)\n",
        "reduced_down_model = keras.Model(inputs=ext_down_base_reduced_layer.inputs,outputs= ext_down_base_reduced_layer.layers[ext_layer].output)\n",
        "reduced_down_model.save('/content/drive/MyDrive/VERSION5/pruned_models/down_class_code_base_pruned_2layers_v2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RHm-R1kXVZj"
      },
      "source": [
        "Phase ground - Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHcfk26LXSSi",
        "outputId": "e6930103-2b91-4994-cffb-b8d8f4eaf100"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "ext_layer = -6\n",
        "ext_phase_ground_down_base = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_InceptionTime_PHASE_GROUND_DOWN.h5'\n",
        "ext_phase_ground_down_base_reduced_layer =  keras.models.load_model(ext_phase_ground_down_base)\n",
        "reduced_phase_ground_down_model = keras.Model(inputs=ext_phase_ground_down_base_reduced_layer.inputs,outputs= ext_phase_ground_down_base_reduced_layer.layers[ext_layer].output)\n",
        "reduced_phase_ground_down_model.save('/content/drive/MyDrive/VERSION5/pruned_models/down_phase_ground_code_base_pruned_6layers.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRTHh8sHYfoC"
      },
      "source": [
        "Not-Down Stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCK-9GG1YWnv",
        "outputId": "4eef3442-cfde-4523-a584-241b29b8b867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "ext_layer = -2\n",
        "ext_not_down_base = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_InceptionTime_not_down_base_v2.h5'\n",
        "ext_not_down_base_reduced_layer =  tf.keras.models.load_model(ext_not_down_base)\n",
        "reduced_not_down_model = keras.Model(inputs=ext_not_down_base_reduced_layer.inputs,outputs= ext_not_down_base_reduced_layer.layers[ext_layer].output)\n",
        "reduced_not_down_model.save('/content/drive/MyDrive/VERSION5/pruned_models/not_down_class_code_base_pruned_2layers_v2.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKYpNRB_ZDfW"
      },
      "source": [
        "Phase Ground - Not Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSHbl7sFYd46",
        "outputId": "557675f4-1bab-4ffb-fec4-b5e2695a6cce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "ext_layer = -6\n",
        "ext_phase_ground_not_down_base = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_InceptionTime_PHASE_GROUND_NOT_DOWN.h5'\n",
        "ext_phase_ground_not_down_base_reduced_layer =  keras.models.load_model(ext_phase_ground_not_down_base)\n",
        "reduced_phase_ground_not_down_model = keras.Model(inputs=ext_phase_ground_not_down_base_reduced_layer.inputs,outputs= ext_phase_ground_not_down_base_reduced_layer.layers[ext_layer].output)\n",
        "reduced_phase_ground_not_down_model.save('/content/drive/MyDrive/VERSION5/pruned_models/not_down_phase_ground_code_base_pruned_6layers.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4FCEEhwgk_0z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHoKFKVlc8VQ"
      },
      "source": [
        "## Step 2- ONNX conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YerLFZwgdEiJ"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime\n",
        "!pip install -U tf2onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6nUzhoCdP9p"
      },
      "source": [
        "POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJB-D_vdjWu0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHT9qTx1c7pn"
      },
      "outputs": [],
      "source": [
        "pos_ext = '/content/drive/MyDrive/VERSION5/pruned_models/pos_code_base_pruned_2layers.h5'\n",
        "pos_extr =  keras.models.load_model(pos_ext)\n",
        "tf.saved_model.save(pos_extr, \"pos_extracted_model\")\n",
        "!python -m tf2onnx.convert --saved-model pos_extr --output /content/drive/MyDrive/VERSION5/onnx_models/pos_extracted_model.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSNdQ1DHjZSX"
      },
      "outputs": [],
      "source": [
        "pos_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_pos_SimpleGRU256.h5'\n",
        "pos_gru_sliding =  keras.models.load_model(pos_gru)\n",
        "tf.saved_model.save(pos_gru_sliding, \"pos_gru_sliding\")\n",
        "!python -m tf2onnx.convert --saved-model pos_gru_sliding --output /content/drive/MyDrive/VERSION5/onnx_models/pos_gru_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Naem2k3k-t2"
      },
      "source": [
        "Twacs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEjB20YCk-al"
      },
      "outputs": [],
      "source": [
        "twacs_ext = '/content/drive/MyDrive/VERSION5/pruned_models/twacs_code_base_pruned_2layers.h5'\n",
        "twacs_extr =  keras.models.load_model(twacs_ext)\n",
        "tf.saved_model.save(twacs_extr, \"twacs_extr\")\n",
        "!python -m tf2onnx.convert --saved-model twacs_extr --output /content/drive/MyDrive/VERSION5/onnx_models/twacs_extracted_model.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vmK1R3ik9oU"
      },
      "outputs": [],
      "source": [
        "twacs_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_SimpleGRU256_TWACS_LSTM.h5'\n",
        "twacs_gru_sliding =  keras.models.load_model(twacs_gru)\n",
        "tf.saved_model.save(twacs_gru_sliding, \"twacs_gru_sliding\")\n",
        "!python -m tf2onnx.convert --saved-model twacs_gru_sliding --output /content/drive/MyDrive/VERSION5/onnx_models/twacs_gru_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESRE91_emY_i"
      },
      "source": [
        "Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtxiYV6RmY1Y"
      },
      "outputs": [],
      "source": [
        "down_ext = '/content/drive/MyDrive/VERSION5/pruned_models/down_class_code_base_pruned_2layers_v2.h5'\n",
        "down_extr =  keras.models.load_model(down_ext)\n",
        "tf.saved_model.save(down_extr, \"down_extr\")\n",
        "!python -m tf2onnx.convert --saved-model down_extr --output /content/drive/MyDrive/VERSION5/onnx_models/downstream_extracted_model_v2.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNjkVu-emYmz"
      },
      "outputs": [],
      "source": [
        "down_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_down_SimpleGRU256.h5'\n",
        "down_gru_sliding =  keras.models.load_model(down_gru)\n",
        "tf.saved_model.save(down_gru_sliding, \"down_gru_sliding\")\n",
        "!python -m tf2onnx.convert --saved-model down_gru_sliding --output /content/drive/MyDrive/VERSION5/onnx_models/down_gru_model_v2.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN8mumBQnsrH"
      },
      "source": [
        "Down Phase Ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_31Byd2Knsf4"
      },
      "outputs": [],
      "source": [
        "down_phase_ext = '/content/drive/MyDrive/VERSION5/pruned_models/down_phase_ground_code_base_pruned_6layers.h5'\n",
        "down_phase_extr =  keras.models.load_model(down_phase_ext)\n",
        "tf.saved_model.save(down_phase_extr, \"down_phase_extr\")\n",
        "!python -m tf2onnx.convert --saved-model down_phase_extr --output /content/drive/MyDrive/VERSION5/onnx_models/downstream_phase_extracted_model.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skmtGXaansI_"
      },
      "outputs": [],
      "source": [
        "down_phase_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_SimpleGRU256_PHASE_GROUND_down.h5'\n",
        "down_phase_gru_sliding =  keras.models.load_model(down_phase_gru)\n",
        "tf.saved_model.save(down_phase_gru_sliding, \"down_phase_gru_sliding\")\n",
        "!python -m tf2onnx.convert --saved-model down_phase_gru_sliding --output /content/drive/MyDrive/VERSION5/onnx_models/down_phase_gru_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sn6ntKXXnaTj"
      },
      "source": [
        "Not Down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TMDflKaEmYgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35579ed6-eb70-4eb2-bca6-da4a858efd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 85). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-23 23:35:29.010158: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-23 23:35:29.010305: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-23 23:35:29.010324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/usr/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-03-23 23:35:31,376 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-03-23 23:35:38,844 - INFO - Signatures found in model: [serving_default].\n",
            "2023-03-23 23:35:38,844 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-03-23 23:35:38,846 - INFO - Output names: ['global_average_pooling1d']\n",
            "2023-03-23 23:35:41,138 - INFO - Using tensorflow=2.11.0, onnx=1.13.1, tf2onnx=1.13.0/2c1db5\n",
            "2023-03-23 23:35:41,138 - INFO - Using opset <onnx, 13>\n",
            "2023-03-23 23:35:41,330 - INFO - Computed 0 values for constant folding\n",
            "2023-03-23 23:35:41,875 - INFO - Optimizing ONNX model\n",
            "2023-03-23 23:35:46,326 - INFO - After optimization: Cast -101 (101->0), Const -317 (447->130), GlobalAveragePool +1 (0->1), Identity -2 (2->0), ReduceMean -1 (1->0), Reshape -101 (101->0), Squeeze -15 (101->86), Transpose -155 (202->47), Unsqueeze -53 (101->48)\n",
            "2023-03-23 23:35:46,697 - INFO - \n",
            "2023-03-23 23:35:46,697 - INFO - Successfully converted TensorFlow model not_down_extr to ONNX\n",
            "2023-03-23 23:35:46,697 - INFO - Model inputs: ['input_1']\n",
            "2023-03-23 23:35:46,697 - INFO - Model outputs: ['global_average_pooling1d']\n",
            "2023-03-23 23:35:46,697 - INFO - ONNX model is saved at /content/drive/MyDrive/VERSION5/onnx_models/not_downstream_extracted_model_v2.onnx\n"
          ]
        }
      ],
      "source": [
        "not_down_ext = '/content/drive/MyDrive/VERSION5/pruned_models/not_down_class_code_base_pruned_2layers_v2.h5'\n",
        "              # '/content/drive/MyDrive/VERSION5/pruned_models/not_down_class_code_base_pruned_2layers.h5'\n",
        "              # \"/content/drive/MyDrive/VERSION5/pruned_models/not_down_class_code_base_pruned_2layers_next.h5\"\n",
        "not_down_extr =  keras.models.load_model(not_down_ext)\n",
        "tf.saved_model.save(not_down_extr, \"not_down_extr\")\n",
        "!python -m tf2onnx.convert --saved-model not_down_extr --output /content/drive/MyDrive/VERSION5/onnx_models/not_downstream_extracted_model_v2.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUu4gqlTmYco",
        "outputId": "b4df5752-2794-4f52-944e-07c1f41f35d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-23 23:37:36.478728: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-23 23:37:36.478854: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-03-23 23:37:36.478878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/usr/lib/python3.9/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-03-23 23:37:38,608 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-03-23 23:37:40,944 - INFO - Signatures found in model: [serving_default].\n",
            "2023-03-23 23:37:40,944 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-03-23 23:37:40,944 - INFO - Output names: ['dense_1']\n",
            "2023-03-23 23:37:41,410 - INFO - Using tensorflow=2.11.0, onnx=1.13.1, tf2onnx=1.13.0/2c1db5\n",
            "2023-03-23 23:37:41,410 - INFO - Using opset <onnx, 13>\n",
            "2023-03-23 23:37:41,464 - INFO - Computed 0 values for constant folding\n",
            "2023-03-23 23:37:41,469 - INFO - Computed 0 values for constant folding\n",
            "2023-03-23 23:37:41,530 - INFO - Computed 0 values for constant folding\n",
            "2023-03-23 23:37:41,626 - INFO - Optimizing ONNX model\n",
            "2023-03-23 23:37:41,772 - INFO - After optimization: Cast -1 (4->3), Const -15 (29->14), Identity -2 (2->0), Shape -1 (2->1), Squeeze -1 (3->2), Unsqueeze -2 (3->1)\n",
            "2023-03-23 23:37:41,787 - INFO - \n",
            "2023-03-23 23:37:41,788 - INFO - Successfully converted TensorFlow model not_down_gru_sliding_updated to ONNX\n",
            "2023-03-23 23:37:41,788 - INFO - Model inputs: ['input_1']\n",
            "2023-03-23 23:37:41,788 - INFO - Model outputs: ['dense_1']\n",
            "2023-03-23 23:37:41,788 - INFO - ONNX model is saved at /content/drive/MyDrive/VERSION5/onnx_models/not_down_gru_model_v2.onnx\n"
          ]
        }
      ],
      "source": [
        "not_down_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_notdown_SimpleGRU256.h5'\n",
        "# not_down_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_notdownSimpleGRU256.h5'\n",
        "not_down_gru_sliding =  keras.models.load_model(not_down_gru)\n",
        "tf.saved_model.save(not_down_gru_sliding, \"not_down_gru_sliding_updated\")\n",
        "!python -m tf2onnx.convert --saved-model not_down_gru_sliding_updated --output /content/drive/MyDrive/VERSION5/onnx_models/not_down_gru_model_v2.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sx1z08bqgAS"
      },
      "source": [
        "Not Down- Phase Ground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_3x7Fh4qm1A"
      },
      "outputs": [],
      "source": [
        "not_down_phase_ext = '/content/drive/MyDrive/VERSION5/pruned_models/not_down_phase_ground_code_base_pruned_6layers.h5'\n",
        "not_down_phase_extr =  keras.models.load_model(not_down_phase_ext)\n",
        "tf.saved_model.save(not_down_phase_extr, \"not_down_phase_extr\")\n",
        "!python -m tf2onnx.convert --saved-model not_down_phase_extr --output /content/drive/MyDrive/VERSION5/onnx_models/not_downstream_phase_extracted_model.onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6SYIu9hqmqE",
        "outputId": "bb905b8b-527b-41d0-9a73-afb21cedd60b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_6_layer_call_fn, gru_cell_6_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-23 00:34:02.926102: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-23 00:34:02.926239: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-23 00:34:02.926264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/usr/lib/python3.8/runpy.py:127: RuntimeWarning: 'tf2onnx.convert' found in sys.modules after import of package 'tf2onnx', but prior to execution of 'tf2onnx.convert'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "2023-02-23 00:34:05,523 - WARNING - '--tag' not specified for saved_model. Using --tag serve\n",
            "2023-02-23 00:34:06,978 - INFO - Signatures found in model: [serving_default].\n",
            "2023-02-23 00:34:06,978 - WARNING - '--signature_def' not specified, using first signature: serving_default\n",
            "2023-02-23 00:34:06,978 - INFO - Output names: ['dense_3']\n",
            "2023-02-23 00:34:07,200 - INFO - Using tensorflow=2.11.0, onnx=1.13.1, tf2onnx=1.13.0/2c1db5\n",
            "2023-02-23 00:34:07,201 - INFO - Using opset <onnx, 13>\n",
            "2023-02-23 00:34:07,237 - INFO - Computed 0 values for constant folding\n",
            "2023-02-23 00:34:07,244 - INFO - Computed 0 values for constant folding\n",
            "2023-02-23 00:34:07,263 - INFO - Computed 0 values for constant folding\n",
            "2023-02-23 00:34:07,332 - INFO - Optimizing ONNX model\n",
            "2023-02-23 00:34:07,412 - INFO - After optimization: Cast -1 (4->3), Const -15 (29->14), Identity -2 (2->0), Shape -1 (2->1), Squeeze -1 (3->2), Unsqueeze -2 (3->1)\n",
            "2023-02-23 00:34:07,424 - INFO - \n",
            "2023-02-23 00:34:07,425 - INFO - Successfully converted TensorFlow model not_down_phase_gru_sliding to ONNX\n",
            "2023-02-23 00:34:07,425 - INFO - Model inputs: ['input_2']\n",
            "2023-02-23 00:34:07,425 - INFO - Model outputs: ['dense_3']\n",
            "2023-02-23 00:34:07,425 - INFO - ONNX model is saved at /content/drive/MyDrive/VERSION5/onnx_models/not_down_phase_gru_model.onnx\n"
          ]
        }
      ],
      "source": [
        "not_down_phase_gru = '/content/drive/MyDrive/VERSION5/LSTM_models/best_VERSION5_SimpleGRU256_PHASE_GROUND_notdown.h5'\n",
        "not_down_phase_gru_sliding =  keras.models.load_model(not_down_phase_gru)\n",
        "tf.saved_model.save(not_down_phase_gru_sliding, \"not_down_phase_gru_sliding\")\n",
        "!python -m tf2onnx.convert --saved-model not_down_phase_gru_sliding --output /content/drive/MyDrive/VERSION5/onnx_models/not_down_phase_gru_model.onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-1Cm0oaf9P"
      },
      "source": [
        "## -----------------------------------------------------------TEST ----------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbob3Wa5tGKy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import  glob\n",
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "import pickle\n",
        "import sys\n",
        "import datetime\n",
        "import uuid\n",
        "import os\n",
        "import subprocess\n",
        "import os, psutil\n",
        "import logging\n",
        "import onnxruntime as ort\n",
        "\n",
        "# logging.basicConfig(filename = '/content/dfa_ml.log',level=logging.DEBUG)#,encoding = 'utf-8'\n",
        "\n",
        "logging.basicConfig(filename = '/Users/apple/Documents/work_DFA/deployment/dfa_ml.log',level=logging.DEBUG)#,encoding = 'utf-8'\n",
        "name = '/content/118004001001_20230307_131124.pqd.bin'\n",
        "\n",
        "as_strided = np.lib.stride_tricks.as_strided\n",
        "mask_1 = np.array(range(4,9))\n",
        "mask_2 = np.array(range(13,37))\n",
        "\n",
        "user_ids = [1000008,1000009]\n",
        "prev_version_models= ['ext_pos','tree_model_pos',\n",
        "                      'ext_twacs','model_twacs',\n",
        "                      'ext_dwn','model_dwn',\n",
        "                      'ext_phase_dwn','model_phase_dwn',\n",
        "                      'ext_not_dwn','model_not_dwn',\n",
        "                      'ext_phase_not_dwn','model_phase_not_dwn']\n",
        "current_version_models = ['pos_extracted_model','pos_gru_model',\n",
        "                      'twacs_extracted_model','twacs_gru_model',\n",
        "                      'downstream_extracted_model','down_gru_model',\n",
        "                      'downstream_phase_extracted_model','down_phase_gru_model',\n",
        "                      'not_downstream_extracted_model','not_down_gru_model',\n",
        "                      'not_downstream_phase_extracted_model','not_down_phase_gru_model']\n",
        "\n",
        "bin_location = '/dfa/waveforms/'\n",
        "## data ingest\n",
        "# name = sys.argv[1]\n",
        "# waveform_id = sys.argv[3]\n",
        "# monitor_id = sys.argv[2]\n",
        "# row_id = sys.argv[4]\n",
        "# guid = uuid.uuid4()\n",
        "\n",
        "file = open(name, \"rb\")\n",
        "data = np.fromfile(file, '<f4')  \n",
        "print('Data shape : '+ str(data.shape))\n",
        "arr_size = data.shape[0]\n",
        "data1 = []\n",
        "data1.append(data)\n",
        "\n",
        "down = [0,1]\n",
        "\n",
        "# file_pat =  '*steps_50offset.npy'\n",
        "dwn_labels = np.array( ['101100', '101200', '121100', '131100', '131200', '131210',\n",
        "                        '131700', '151100', '201300', '201400', '202100', '205100',\n",
        "                        '205200', '231000', '231900', '233100', '302000'])\n",
        "not_dwn_labels = np.array(['101101', '101201', '121101',\n",
        "                           '131701', '132101', '132201', \n",
        "                           '133101', '151101', '202101'] )\n",
        "#[A B C AB BC CA ABC AB BC CA ABC]\n",
        "dwn_phase_labels     = np.array([0, 1, 2, 3, 4, 5, 6, 3, 4, 5, 6])\n",
        "#[A B C AB BC CA ABC ABN]\n",
        "not_dwn_phase_labels = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "#[AN BN CN AB BC CA ABC ABN BCN CAN ABCN]\n",
        "dwn_ground_labels     = np.array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])\n",
        "# Ground is always unk for non downstream\n",
        "not_dwn_ground_labels = np.array([3, 3, 3, 3, 3, 3, 3])\n",
        "\n",
        "class Indx(IntEnum):\n",
        "    ID = 0\n",
        "    MONITOR = 1\n",
        "    WAVEFORM = 2\n",
        "    FILE = 3\n",
        "    CLASS = 4\n",
        "    POSITION = 5\n",
        "    PHASE = 6\n",
        "    GROUND = 7\n",
        "    FREQ = 8\n",
        "    SECONDS = 9\n",
        "    FLAG = 10\n",
        "    DELTA_PT = 11\n",
        "    THREE_WIRE = 12\n",
        "    BLOB = 13\n",
        "    RANDOM = 14\n",
        "    LAST_CHANNEL = 31\n",
        "class SIndx(IntEnum):\n",
        "    ID = 0\n",
        "    NAME = 1\n",
        "    PT_S_A = 2\n",
        "    PT_S_B = 3\n",
        "    PT_S_C = 4\n",
        "    PT_P_A = 5\n",
        "    PT_P_B = 6\n",
        "    PT_P_C = 7\n",
        "    CT_S_A = 8\n",
        "    CT_S_B = 9\n",
        "    CT_S_C = 10\n",
        "    CT_P_A = 11\n",
        "    CT_P_B = 12\n",
        "    CT_P_C = 13\n",
        "    VA_SCALE = 14\n",
        "    VB_SCALE = 15\n",
        "    VC_SCALE = 16\n",
        "    IA_SCALE = 17\n",
        "    IB_SCALE = 18\n",
        "    IC_SCALE = 19\n",
        "    PQ_SCALE = 99\n",
        "    NO_SCALE = 100\n",
        "class Channels(IntEnum):\n",
        "    VA    =    0\n",
        "    VB    =    1\n",
        "    VC    =    2\n",
        "    IA    =    3\n",
        "    IB    =    4\n",
        "    IC    =    5\n",
        "    IN    =    6\n",
        "    VA_PD =    7\n",
        "    VB_PD =    8\n",
        "    VC_PD =    9\n",
        "    IA_PD =    10\n",
        "    IB_PD =    11\n",
        "    IC_PD =    12\n",
        "    IN_PD =    13\n",
        "    PA =       14\n",
        "    PB =       15\n",
        "    PC =       16\n",
        "    QA =       17\n",
        "    QB =       18\n",
        "    QC =       19\n",
        "    VA_SD =    20\n",
        "    VB_SD =    21\n",
        "    VC_SD =    22\n",
        "    IA_SD =    23\n",
        "    IB_SD =    24\n",
        "    IC_SD =    25\n",
        "    IN_SD =    26\n",
        "    VA_EVENS = 27\n",
        "    VB_EVENS = 28\n",
        "    VC_EVENS = 29\n",
        "    IA_EVENS = 30\n",
        "    IB_EVENS = 31\n",
        "    IC_EVENS = 32\n",
        "    IN_EVENS = 33\n",
        "    VA_ODDS =  34\n",
        "    VB_ODDS =  35\n",
        "    VC_ODDS =  36\n",
        "    IA_ODDS =  37\n",
        "    IB_ODDS =  38\n",
        "    IC_ODDS =  39\n",
        "    IN_ODDS =  40\n",
        "    \n",
        "    VA_NONS = 41\n",
        "    VB_NONS = 42\n",
        "    VC_NONS = 43\n",
        "    IA_NONS = 44\n",
        "    IB_NONS = 45\n",
        "    IC_NONS = 46\n",
        "    IN_NONS = 47\n",
        "    VAB = 48\n",
        "    VBC = 49\n",
        "    VCA = 50\n",
        "    \n",
        "class ce(IntEnum):\n",
        "    v = 0\n",
        "    i = 3\n",
        "    v_pd = 6\n",
        "    i_pd = 9\n",
        "    v_sd = 12\n",
        "    i_sd = 15\n",
        "    v_evens = 18\n",
        "    i_evens = 21\n",
        "    v_odds = 24\n",
        "    i_odds = 27\n",
        "    p = 30\n",
        "    q = 33\n",
        "\n",
        "class used_chnls(IntEnum):\n",
        "    VA    =    0\n",
        "    VB    =    1\n",
        "    VC    =    2\n",
        "    IA    =    3\n",
        "    IB    =    4\n",
        "    IC    =    5\n",
        "    IN    =    6\n",
        "    VA_PD =    7\n",
        "    VB_PD =    8\n",
        "    VC_PD =    9\n",
        "    IA_PD =    10\n",
        "    IB_PD =    11\n",
        "    IC_PD =    12\n",
        "    IN_PD =    13\n",
        "    IA_EVENS = 14\n",
        "    IB_EVENS = 15\n",
        "    IC_EVENS = 16\n",
        "    IN_EVENS = 17\n",
        "    PA =       18\n",
        "    PB =       19\n",
        "    PC =       20\n",
        "    QA =       21\n",
        "    QB =       22\n",
        "    QC =       23\n",
        "    IA_NONS = 24\n",
        "    IB_NONS = 25\n",
        "    IC_NONS = 26\n",
        "    IN_NONS = 27\n",
        "    VAB = 28\n",
        "    VBC = 29\n",
        "    VCA = 30\n",
        "\n",
        "def get_raw_vector_bulk(records_in, channel):\n",
        "    arr = np.frombuffer(records_in[-1], dtype=np.float32)\n",
        "    leng = int(arr_size/51)#arr[Indx.SECONDS] * arr[Indx.FREQ]\n",
        "    # array size constant 660 #######change\n",
        "    vect_size  = arr[(leng*channel):(leng*(channel + 1))]\n",
        "    vect = vect_size#[start:end]\n",
        "    return vect\n",
        "\n",
        "def all_channel_input(input_data = data1):\n",
        "    new_result = dict()\n",
        "    channels = ['VA','VB','VC',\n",
        "                'IA','IB','IC','IN',\n",
        "                'VA_PD','VB_PD','VC_PD',\n",
        "                'IA_PD','IB_PD','IC_PD','IN_PD',\n",
        "                'IA_EVENS','IB_EVENS','IC_EVENS','IN_EVENS',\n",
        "                'PA','PB','PC',\n",
        "                'QA','QB','QC',\n",
        "                'IA_NONS','IB_NONS','IC_NONS','IN_NONS',\n",
        "                'VAB','VBC','VCA'           \n",
        "    ]\n",
        "    \n",
        "    rms_raw_vecs = []\n",
        "\n",
        "    for j in channels:\n",
        "        \n",
        "        if j == 'VA':\n",
        "            channel=Channels.VA\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j == 'VB':\n",
        "            channel=Channels.VB\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VC':\n",
        "            channel=Channels.VC\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "        #---\n",
        "        elif j == 'IA':\n",
        "            channel=Channels.IA\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB':\n",
        "            channel=Channels.IB\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC':\n",
        "            channel=Channels.IC\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN':\n",
        "            channel=Channels.IN\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        #---\n",
        "        elif j == 'VA_PD':\n",
        "            channel=Channels.VA_PD\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j == 'VB_PD':\n",
        "            channel=Channels.VB_PD\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VC_PD':\n",
        "            channel=Channels.VC_PD\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "        #---\n",
        "        elif j == 'IA_PD':\n",
        "            channel=Channels.IA_PD\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_PD':\n",
        "            channel=Channels.IB_PD\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_PD':\n",
        "            channel=Channels.IC_PD\n",
        "            scale_indx = SIndx.IC_SCALE    \n",
        "        elif j =='IN_PD':\n",
        "            channel=Channels.IN_PD\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        \n",
        "        #---\n",
        "        elif j == 'IA_EVENS':\n",
        "            channel=Channels.IA_EVENS\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_EVENS':\n",
        "            channel=Channels.IB_EVENS\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_EVENS':\n",
        "            channel=Channels.IC_EVENS\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN_EVENS':\n",
        "            channel=Channels.IN_EVENS\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "            \n",
        "        #---\n",
        "        elif j =='PA':\n",
        "            channel=Channels.PA\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j =='PB':\n",
        "            channel=Channels.PB\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j =='PC':\n",
        "            channel=Channels.PC\n",
        "            scale_indx = SIndx.VC_SCALE                \n",
        "        #---\n",
        "        elif j =='QA':\n",
        "            channel=Channels.QA\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j =='QB':\n",
        "            channel=Channels.QB\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j =='QC':\n",
        "            channel=Channels.QC\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "\n",
        "        #---    \n",
        "        elif j =='IA_NONS':\n",
        "            channel=Channels.IA_NONS\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_NONS':\n",
        "            channel=Channels.IB_NONS\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_NONS':\n",
        "            channel=Channels.IC_NONS\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN_NONS':\n",
        "            channel=Channels.IN_NONS\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        #---\n",
        "        elif j =='VAB':\n",
        "            channel=Channels.VAB\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j =='VBC':\n",
        "            channel=Channels.VBC\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VCA':\n",
        "            channel=Channels.VCA\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "            \n",
        "            \n",
        "                \n",
        "        vect = get_raw_vector_bulk(records_in = data1, channel=channel)\n",
        "        rms_raw_vecs.append(vect)\n",
        "            \n",
        "    return rms_raw_vecs\n",
        "i_params ={\n",
        "    'small': 10,\n",
        "    'med_start': 100,\n",
        "    'med_end': 500,\n",
        "    'big': 1000\n",
        "    }\n",
        "\n",
        "pq_params ={\n",
        "    'small': 10000,\n",
        "    'med_start': 50000,\n",
        "    'med_end': 100000,\n",
        "    'big': 500000\n",
        "    }\n",
        "\n",
        "def fuzzy_scale(val, params):\n",
        "    ret = 0.0\n",
        "    if val <= params['small']:\n",
        "        ret = 0.33\n",
        "    elif val < params['med_start']:\n",
        "        ret  = 0.33 + (val - params['small']) * (0.33/(params['med_start'] - params['small']))\n",
        "    elif val <= params['med_end']:\n",
        "        ret = 0.66\n",
        "    elif val < params['big']:\n",
        "        ret  = 0.66 + (val - params['med_end']) * (0.33/(params['big'] - params['med_end']))\n",
        "    else:\n",
        "        ret = 1.0\n",
        "    return ret\n",
        "\n",
        "############ ends ##############\n",
        "\n",
        "def normalize(rms_vec,new_max = 1):\n",
        "        #change as per requirement, default selected sliced index for test purpose\n",
        "    _all = rms_vec\n",
        "\n",
        "    \n",
        "### new normalization ###\n",
        "    ########################################## VA,VB,VC ##########################\n",
        "    vabc_all = np.stack((_all[used_chnls.VA],_all[used_chnls.VB],_all[used_chnls.VC]))\n",
        "    \n",
        "    vabc_max = np.max(vabc_all)\n",
        "    vabc_min = np.min(vabc_all)\n",
        "    v_range = vabc_max - vabc_min\n",
        "    if new_max == 0:\n",
        "        if v_range < 1e-10 or vabc_max < 1:\n",
        "            print('found vabc_max ' + str(v_range)+' <0 or '+ str(vabc_max)+' <0' )\n",
        "            norm_va,norm_vb,norm_vc = ['x'],['x'],['x']\n",
        "            v_bias = ['x']\n",
        "        else:\n",
        "            #skip the record\n",
        "            # vabc_max if <1 skip this\n",
        "            v_scale = v_range/vabc_max\n",
        "            norm_va = (_all[used_chnls.VA] - vabc_min)/(v_range)\n",
        "            norm_vb = (_all[used_chnls.VB] - vabc_min)/(v_range)\n",
        "            norm_vc = (_all[used_chnls.VC] - vabc_min)/(v_range)\n",
        "            v_bias = [v_scale] * 120\n",
        "    else:\n",
        "        if v_range < 1e-10 or vabc_max < 1:\n",
        "            print('found ' + str(v_range)+' <0 or '+ str(vabc_max)+' <0' )\n",
        "            norm_va,norm_vb,norm_vc = ['x'],['x'],['x']\n",
        "            v_bias = ['x']\n",
        "        else:\n",
        "            v_scale = v_range/vabc_max\n",
        "            norm_va = (_all[used_chnls.VA])/(vabc_max)\n",
        "            norm_vb = (_all[used_chnls.VB])/(vabc_max)\n",
        "            norm_vc = (_all[used_chnls.VC])/(vabc_max)\n",
        "            v_bias = [v_scale] * 120\n",
        "    \n",
        "    ########################################## IA,IB,IC ##########################\n",
        "    iabc_all = np.stack((_all[used_chnls.IA],_all[used_chnls.IB],_all[used_chnls.IC],_all[used_chnls.IN]))\n",
        "    \n",
        "    iabc_max = np.max(iabc_all)\n",
        "    iabc_min = np.min(iabc_all)\n",
        "    i_range = iabc_max - iabc_min #skip the record if < 0.001\n",
        "    if new_max == 0:\n",
        "        if i_range < 1e-20 or iabc_max < 1e-10:\n",
        "            print('found i_range' + str(i_range)+' <0.001'  )\n",
        "            norm_ia,norm_ib,norm_ic,norm_in = ['x'],['x'],['x'],['x']\n",
        "            i_bias = ['x']\n",
        "        else:\n",
        "            i_scale = fuzzy_scale(i_range, i_params)\n",
        "            norm_ia = (_all[used_chnls.IA] - iabc_min)/(i_range)\n",
        "            norm_ib = (_all[used_chnls.IB] - iabc_min)/(i_range)\n",
        "            norm_ic = (_all[used_chnls.IC] - iabc_min)/(i_range)\n",
        "            norm_in = (_all[used_chnls.IN] - iabc_min)/(i_range)\n",
        "            i_bias = [i_scale] * 120\n",
        "    else:\n",
        "        if i_range < 1e-20 or iabc_max < 1e-10:\n",
        "            print('found ' + str(i_range)+' <0.001'  )\n",
        "            norm_ia,norm_ib,norm_ic,norm_in = ['x'],['x'],['x'],['x']\n",
        "            i_bias = ['x']\n",
        "        else:\n",
        "            i_scale = fuzzy_scale(i_range, i_params)\n",
        "            norm_ia = (_all[used_chnls.IA])/(iabc_max)\n",
        "            norm_ib = (_all[used_chnls.IB])/(iabc_max)\n",
        "            norm_ic = (_all[used_chnls.IC])/(iabc_max)\n",
        "            norm_in = (_all[used_chnls.IN])/(iabc_max)\n",
        "            i_bias = [i_scale]* 120\n",
        "            \n",
        "    \n",
        "    \n",
        "    #################################### 'VA_PD','VB_PD','VC_PD'  #############################\n",
        "    PD_vabc_all =  np.stack((_all[used_chnls.VA_PD],_all[used_chnls.VB_PD],_all[used_chnls.VC_PD]))\n",
        "    \n",
        "    PD_vabc_max = np.max(PD_vabc_all)\n",
        "    PD_vabc_min = np.min(PD_vabc_all)\n",
        "    v_pd_scale = PD_vabc_max/vabc_max #vabc_max<1 skip and PD_vabc_max< 1\n",
        "    if PD_vabc_max < 1e-10:\n",
        "        print('found PD_vabc_max',str(vabc_max),str(PD_vabc_max), ' < 1'  )\n",
        "        norm_va_pd,norm_vb_pd,norm_vc_pd = ['x'],['x'],['x']\n",
        "        vpd_bias = ['x'] \n",
        "    else: \n",
        "        norm_va_pd = (_all[used_chnls.VA_PD] )/( PD_vabc_max)\n",
        "        norm_vb_pd = (_all[used_chnls.VB_PD] )/( PD_vabc_max)\n",
        "        norm_vc_pd = (_all[used_chnls.VC_PD] )/( PD_vabc_max)\n",
        "        vpd_bias = [v_pd_scale]* 120\n",
        "    \n",
        "    ################################  IA_PD,IB_PD,IC_PD,IN_PD  #############################\n",
        "    PD_iabc_all = np.stack((_all[used_chnls.IA_PD],_all[used_chnls.IB_PD],_all[used_chnls.IC_PD],_all[used_chnls.IN_PD]))\n",
        "    \n",
        "    PD_iabc_max = np.max(PD_iabc_all)\n",
        "    PD_iabc_min = np.min(PD_iabc_all)\n",
        "    if PD_iabc_max<1e-10:\n",
        "        print('found PD_iabc_max ' + str(PD_iabc_max)+' <0.001'  )\n",
        "        norm_ia_pd,norm_ib_pd,norm_ic_pd,norm_ic_pd = ['x'],['x'],['x'],['x']\n",
        "        ipd_bias = ['x'] \n",
        "    else:\n",
        "        i_pd_scale = fuzzy_scale(PD_iabc_max, i_params)\n",
        "        norm_ia_pd = (_all[used_chnls.IA_PD] )/(PD_iabc_max)# PD_iabc_max <0.001\n",
        "        norm_ib_pd = (_all[used_chnls.IB_PD])/(PD_iabc_max)\n",
        "        norm_ic_pd = (_all[used_chnls.IC_PD])/(PD_iabc_max)\n",
        "        norm_in_pd = (_all[used_chnls.IN_PD])/(PD_iabc_max)\n",
        "        ipd_bias = [i_pd_scale]* 120\n",
        "    \n",
        "    ################################# I_EVEN & I_ODD  ####################################\n",
        "    ## I_EVEN\n",
        "    even_iabc_all = np.stack((_all[used_chnls.IA_EVENS],_all[used_chnls.IB_EVENS],_all[used_chnls.IC_EVENS],_all[used_chnls.IN_EVENS]))\n",
        "    \n",
        "    even_iabc_max = np.max(even_iabc_all)\n",
        "    even_iabc_min = np.min(even_iabc_all)\n",
        "    if even_iabc_max<1e-10:\n",
        "        print('found even_iabc_max ' + str(even_iabc_all)+' <0.e-10'  )\n",
        "        norm_ia_even,norm_ib_even,norm_ic_even,norm_in_even = ['x'],['x'],['x'],['x']\n",
        "        ieven_bias = ['x'] \n",
        "    else:\n",
        "        i_even_scale = fuzzy_scale(even_iabc_max, i_params)\n",
        "        norm_ia_even = (_all[used_chnls.IA_EVENS] )/(even_iabc_max)# even_iabc_max <0.001\n",
        "        norm_ib_even = (_all[used_chnls.IB_EVENS])/(even_iabc_max)\n",
        "        norm_ic_even = (_all[used_chnls.IC_EVENS])/(even_iabc_max)\n",
        "        norm_in_even = (_all[used_chnls.IN_EVENS])/(even_iabc_max)\n",
        "        ieven_bias = [i_even_scale] * 120\n",
        "        \n",
        "    #######################################   PA,PB,PC  ##################################\n",
        "    pabc_all = np.stack((_all[used_chnls.PA],_all[used_chnls.PB],_all[used_chnls.PC]))\n",
        "    \n",
        "    pabc_min = np.min(pabc_all)\n",
        "    pabc_max = np.max(pabc_all)\n",
        "    p_range = pabc_max - pabc_min # p_range<1 skip\n",
        "    if p_range<0.001:\n",
        "        print('found p_range' + str(p_range)+' <1'  )\n",
        "        norm_pa,norm_pb,norm_pc = ['x'],['x'],['x']\n",
        "        p_bias = ['x']\n",
        "    else:\n",
        "        p_scale = fuzzy_scale(p_range, pq_params)\n",
        "        norm_pa = (_all[used_chnls.PA] - float(pabc_min))/p_range\n",
        "        norm_pb = (_all[used_chnls.PB] - float(pabc_min))/p_range\n",
        "        norm_pc = (_all[used_chnls.PC] - float(pabc_min))/p_range\n",
        "        p_bias = [p_scale] * 120\n",
        "\n",
        "    # QA,QB,QC\n",
        "    qabc_all = np.stack((_all[used_chnls.QA],_all[used_chnls.QB],_all[used_chnls.QC]))\n",
        "    qabc_min = np.min(qabc_all)\n",
        "    qabc_max = np.max(qabc_all)\n",
        "    q_range = qabc_max - qabc_min # q_range<1 skip \n",
        "    if q_range< 0.001:\n",
        "        print('found q_range' + str(q_range)+' <1'  )\n",
        "        norm_qa,norm_qb,norm_qc = ['x'],['x'],['x']\n",
        "        q_bias = ['x'] \n",
        "    else:  \n",
        "        q_scale = fuzzy_scale(q_range, pq_params)\n",
        "        norm_qa= (_all[used_chnls.QA] - qabc_min)/q_range\n",
        "        norm_qb= (_all[used_chnls.QB] - qabc_min)/q_range\n",
        "        norm_qc= (_all[used_chnls.QC] - qabc_min)/q_range\n",
        "        q_bias = [q_scale] * 120\n",
        "    \n",
        "    ########################## 'IA_NONS','IB_NONS','IC_NONS','IN_NONS' #########################\n",
        "    nons_iabc_all = np.stack((_all[used_chnls.IA_NONS],_all[used_chnls.IB_NONS],_all[used_chnls.IC_NONS],_all[used_chnls.IN_NONS]))\n",
        "\n",
        "    nons_iabc_max = np.max(nons_iabc_all)\n",
        "    nons_iabc_min = np.min(nons_iabc_all)\n",
        "    if nons_iabc_max<0.001:\n",
        "        print('found ' + str(nons_iabc_max)+' <0.001'  )\n",
        "        norm_ia_nons,norm_ib_nons,norm_ic_nons,norm_in_nons = ['x'],['x'],['x'],['x']\n",
        "        inons_bias = ['x'] \n",
        "    else:\n",
        "        i_nons_scale = fuzzy_scale(nons_iabc_max, i_params)\n",
        "        norm_ia_nons = (_all[used_chnls.IA_NONS] )/(nons_iabc_max)\n",
        "        norm_ib_nons = (_all[used_chnls.IB_NONS])/(nons_iabc_max)\n",
        "        norm_ic_nons = (_all[used_chnls.IC_NONS])/(nons_iabc_max)\n",
        "        norm_in_nons = (_all[used_chnls.IN_NONS])/(nons_iabc_max)\n",
        "        inons_bias = [i_nons_scale] * 120\n",
        "        \n",
        "    ##################################### 'VAB','VBC','VCA'  ###################################\n",
        "    vabc_ll_all =  np.stack((_all[used_chnls.VAB],_all[used_chnls.VBC],_all[used_chnls.VCA]))\n",
        "    vabc_ll_max = np.max(vabc_ll_all)\n",
        "    vabc_ll_min = np.min(vabc_ll_all)\n",
        "    v_ll_scale = vabc_ll_max/vabc_max #skip vabc_ll_max < 1\n",
        "    if vabc_ll_max<1:\n",
        "        print('found vabc_ll_max '+str(vabc_ll_max)+' <1'  )\n",
        "        norm_va_ll,norm_vb_ll,norm_vc_ll = ['x'],['x'],['x']\n",
        "        vll_bias = ['x'] \n",
        "    else: \n",
        "        norm_va_ll = (_all[used_chnls.VAB] )/( vabc_ll_max)\n",
        "        norm_vb_ll = (_all[used_chnls.VBC] )/( vabc_ll_max)\n",
        "        norm_vc_ll = (_all[used_chnls.VCA] )/( vabc_ll_max)\n",
        "        vll_bias = [v_ll_scale] * 120\n",
        "\n",
        "    ################################## ARRAY STACK ####################################\n",
        "    \n",
        "    bias_list = np.array([np.array(v_bias),np.array(i_bias),\n",
        "                     np.array(vpd_bias) ,np.array(ipd_bias) ,np.array(ieven_bias) ,\n",
        "                     np.array(p_bias),np.array(q_bias) ,\n",
        "                     np.array(inons_bias),np.array(vll_bias)])\n",
        "        \n",
        "        \n",
        "    final_norm = np.array([ norm_va,norm_vb,norm_vc,# np.array(v_bias)\n",
        "                       norm_ia,norm_ib,norm_ic, norm_in,#np.array(i_bias)\n",
        "                       norm_va_pd,norm_vb_pd,norm_vc_pd,# np.array(vpd_bias)\n",
        "                       norm_ia_pd,norm_ib_pd,norm_ic_pd,norm_in_pd, #np.array(ipd_bias)\n",
        "                       norm_ia_even,norm_ib_even,norm_ic_even,norm_in_even,#np.array(ieven_bias)\n",
        "                       norm_pa,norm_pb,norm_pc,# np.array(p_bias)\n",
        "                       norm_qa,norm_qb,norm_qc,#  np.array(q_bias)\n",
        "                       norm_ia_nons,norm_ib_nons,norm_ic_nons,norm_in_nons,\n",
        "                       norm_va_ll,norm_vb_ll,norm_vc_ll\n",
        "                    ])\n",
        "    return final_norm, bias_list\n",
        "\n",
        "def extract_tensor(data,sesson):\n",
        "    sess = sesson\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    output_name = sess.get_outputs()[0].name\n",
        "    res = sess.run([output_name], {input_name: data.astype(np.float32)})\n",
        "    return res[0]\n",
        "\n",
        "def pyt_predict_phase(input_name,sesson):\n",
        "    sess = sesson\n",
        "    input1 = input_name\n",
        "    input_name1 = sess.get_inputs()[0].name\n",
        "    input_name2 = sess.get_inputs()[1].name\n",
        "    output_name = sess.get_outputs()[0].name\n",
        "    res = sess.run([output_name],{input_name1:input1[1].astype(np.float32),input_name2:input1[0].astype(np.float32)})\n",
        "    return res[0]\n",
        "def predict_label():\n",
        "    predictions = []\n",
        "    \n",
        "    user_id0 = user_ids[0]\n",
        "    root_model_path0= '/content/drive/MyDrive/capstone/Feb2023_deployment/version1/'\n",
        "    version0=prev_version_models\n",
        "    \n",
        "    user_id1 = user_ids[1]\n",
        "    root_model_path1= '/content/drive/MyDrive/capstone/Feb2023_deployment/version2/'\n",
        "    version1=current_version_models\n",
        "    \n",
        "    process1 = psutil.Process(os.getpid())\n",
        "    vect = get_raw_vector_bulk(data1,channel = Channels.VA) \n",
        "    logging.debug('process1 - {}' .format(process1.memory_info().rss)) \n",
        "\n",
        "    process2 = psutil.Process(os.getpid())\n",
        "    rms_raw_vecs = all_channel_input(data1)\n",
        "    logging.debug('process2 - {}' .format(process2.memory_info().rss))\n",
        "\n",
        "    process3 = psutil.Process(os.getpid())\n",
        "    scaled_x,bias_x = np.array(normalize(rms_vec=rms_raw_vecs))\n",
        "    logging.debug('process3 - {}' .format(process3.memory_info().rss)) \n",
        "    # Check for nans\n",
        "    is_nan = len(np.argwhere(np.isnan(bias_x)))\n",
        "    if is_nan == 0:\n",
        "        for xx in scaled_x :\n",
        "            is_nan = len(np.argwhere(np.isnan(xx))) \n",
        "            if is_nan > 0:\n",
        "                break\n",
        "\n",
        "    # Proceed on no nans\n",
        "    if is_nan == 0:\n",
        "      # Adding the multi step embedded features needed for LSTM\n",
        "        len_x = 120 * (len(scaled_x[0]) // 120)\n",
        "\n",
        "        #Compute number of possible time steps\n",
        "        starts = np.array(range(0,len_x - 120 + 1, 50))\n",
        "        ts = len(starts)\n",
        "\n",
        "\n",
        "      # Gather data in time steps\n",
        "        main_list = []\n",
        "        for start in starts:\n",
        "            end = int(start)+120\n",
        "            final_norm = np.vstack((bias_x[0], scaled_x[0][start:end],scaled_x[1][start:end],scaled_x[2][start:end],# np.array(v_bias_x)\n",
        "                          bias_x[1] , scaled_x[3][start:end],scaled_x[4][start:end],scaled_x[5][start:end], scaled_x[6][start:end],#np.array(i_bias_x)\n",
        "                            bias_x[2]  , scaled_x[7][start:end],scaled_x[8][start:end],scaled_x[9][start:end],# np.array(vpd_bias_x)\n",
        "                              bias_x[3] , scaled_x[10][start:end],scaled_x[11][start:end],scaled_x[12][start:end],scaled_x[13][start:end], #np.array(ipd_bias_x)\n",
        "                                  bias_x[4] , scaled_x[14][start:end],scaled_x[15][start:end],scaled_x[16][start:end],scaled_x[17][start:end],#np.array(ieven_bias_x)\n",
        "                                          bias_x[5] , scaled_x[18][start:end],scaled_x[19][start:end],scaled_x[20][start:end],# np.array(p_bias_x)\n",
        "                                            bias_x[6] ,  scaled_x[21][start:end],scaled_x[22][start:end],scaled_x[23][start:end],#  np.array(q_bias_x)\n",
        "                                                bias_x[7], scaled_x[24][start:end],scaled_x[25][start:end],scaled_x[26][start:end],scaled_x[27][start:end],\n",
        "                                                  bias_x[8], scaled_x[28][start:end],scaled_x[29][start:end],scaled_x[30][start:end])\n",
        "            ).T\n",
        "            main_list.append(final_norm)\n",
        "        sliced_x = np.array(main_list)\n",
        "        # Determine postion\n",
        "        process4 = psutil.Process(os.getpid())\n",
        "        \n",
        "        sess1_prev = ort.InferenceSession(root_model_path0 + version0[0]+\".onnx\")\n",
        "        sess1_new = ort.InferenceSession(root_model_path1 + version1[0]+\".onnx\")\n",
        "        \n",
        "        pos_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "        pos_x0 = pos_x0.reshape((1, pos_x0.shape[0], pos_x0.shape[1]))\n",
        "        pos_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "        pos_x1 = pos_x1.reshape((1, pos_x1.shape[0], pos_x1.shape[1]))\n",
        "        \n",
        "        sess2_prev = ort.InferenceSession(root_model_path0 + version0[1]+\".onnx\")\n",
        "        sess2_new = ort.InferenceSession(root_model_path1 + version1[1]+\".onnx\")\n",
        "        pos_y0 = extract_tensor(pos_x0,sess2_prev)\n",
        "        pos_y0 = pos_y0.tolist()\n",
        "        pos_y0 = pos_y0[0][0]\n",
        "        print(pos_y0)\n",
        "        pos_y0 = 0 if pos_y0 < 0.5 else 1 \n",
        "        \n",
        "        pos_y1 = extract_tensor(pos_x1,sess2_new)\n",
        "        pos_y1 = pos_y1.tolist()\n",
        "        pos_y1 = pos_y1[0][0]\n",
        "        print(pos_y1)\n",
        "        pos_y1 = 0 if pos_y1 < 0.5 else 1 \n",
        "        logging.debug('process4 - {}' .format(process4.memory_info().rss)) \n",
        "\n",
        "        process5 = psutil.Process(os.getpid())\n",
        "        sess1_prev = ort.InferenceSession(root_model_path0 + version0[2]+\".onnx\")\n",
        "        sess1_new = ort.InferenceSession(root_model_path1 + version1[2]+\".onnx\")\n",
        "        \n",
        "        twacs_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "        twacs_x0 = twacs_x0.reshape((1, twacs_x0.shape[0], twacs_x0.shape[1])) \n",
        "        twacs_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "        twacs_x1 = twacs_x1.reshape((1, twacs_x1.shape[0], twacs_x1.shape[1])) \n",
        "        \n",
        "        sess2_prev = ort.InferenceSession(root_model_path0 + version0[3]+\".onnx\")\n",
        "        sess2_new = ort.InferenceSession(root_model_path1 + version1[3]+\".onnx\")\n",
        "        twacs_y0 = extract_tensor(twacs_x0, sess2_prev)\n",
        "        twacs_y0 = 0 if pos_y0 < 0.7 else 1\n",
        "        twacs_y1 = extract_tensor(twacs_x1, sess2_new)\n",
        "        twacs_y1 = 0 if pos_y1 < 0.7 else 1\n",
        "        \n",
        "        logging.debug('process5 - {}' .format(process5.memory_info().rss)) \n",
        "\n",
        "\n",
        "        if pos_y0 == 0:\n",
        "            process6 = psutil.Process(os.getpid())\n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[4]+\".onnx\")\n",
        "            \n",
        "            dwn_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "            print('prev')\n",
        "            # print(dwn_x0)\n",
        "            dwn_x0 = dwn_x0.reshape((1, dwn_x0.shape[0], dwn_x0.shape[1]))\n",
        "            \n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[5]+\".onnx\") \n",
        "            class_y0= extract_tensor(dwn_x0, sess2_prev)\n",
        "            logging.debug('process6 - {}' .format(process6.memory_info().rss)) \n",
        "                \n",
        "            # intermediate operations to predict phase\n",
        "            process7 = psutil.Process(os.getpid())\n",
        "            indy0 = np.argmax(class_y0)\n",
        "            class_y0 = np.zeros(class_y0.shape)\n",
        "            class_y0[0, indy0] = 1\n",
        "            \n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[6]+\".onnx\")\n",
        "            phase_dwn_x0 = pyt_predict_phase([sliced_x,class_y0],sesson=sess1_prev)\n",
        "            phase_dwn_x0 = np.hstack((np.tile(class_y0, (phase_dwn_x0.shape[0], 1)), phase_dwn_x0))\n",
        "            phase_dwn_x0 = phase_dwn_x0.reshape((1,phase_dwn_x0.shape[0],phase_dwn_x0.shape[1]))\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[7]+\".onnx\")\n",
        "            phase_y0 = extract_tensor(phase_dwn_x0,sess2_prev)\n",
        "            logging.debug('process7 - {}' .format(process7.memory_info().rss))  \n",
        "            \n",
        "            pred_label0 = dwn_labels[class_y0.argmax(axis=1)]\n",
        "            pred_phase0  = dwn_phase_labels[phase_y0.argmax(axis=1)]\n",
        "            pred_ground0 = dwn_ground_labels[phase_y0.argmax(axis=1)]\n",
        "           \n",
        "            predictions.append(pred_label0)\n",
        "            predictions.append(pred_phase0)\n",
        "            predictions.append(pred_ground0)\n",
        "            predictions.append(twacs_y0)\n",
        "            predictions.append(pos_y0)\n",
        "            \n",
        "            # return predictions\n",
        "\n",
        "        elif pos_y0==1:\n",
        "            process8 = psutil.Process(os.getpid())\n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[8]+\".onnx\")\n",
        "            \n",
        "            not_dwn_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "            # print(not_dwn_x0)\n",
        "            not_dwn_x0 = not_dwn_x0.reshape((1, not_dwn_x0.shape[0], not_dwn_x0.shape[1]))  \n",
        "            \n",
        "            \n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[9]+\".onnx\")\n",
        "            \n",
        "            class_y0= extract_tensor(not_dwn_x0, sess2_prev)\n",
        "            logging.debug('process8 - {}' .format(process8.memory_info().rss))\n",
        "\n",
        "            # intermediate operations to predict phase\n",
        "            process9 = psutil.Process(os.getpid())\n",
        "            sliced_x[:,:, mask_1] = 0.0\n",
        "            sliced_x[:,:, mask_2] = 0.0 \n",
        "            \n",
        "            indy0 = np.argmax(class_y0)\n",
        "            class_y0 = np.zeros(class_y0.shape)\n",
        "            class_y0[0, indy0] = 1\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[10]+\".onnx\")\n",
        "            phase_not_dwn_x0 = pyt_predict_phase(input_name=[sliced_x,class_y0],sesson=sess2_prev)\n",
        "            phase_not_dwn_x0 = np.hstack((np.tile(class_y0, (phase_not_dwn_x0.shape[0], 1)), phase_not_dwn_x0))\n",
        "            phase_not_dwn_x0 = phase_not_dwn_x0.reshape((1,phase_not_dwn_x0.shape[0], phase_not_dwn_x0.shape[1]))\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[11]+\".onnx\")\n",
        "            phase_y0 = extract_tensor(phase_not_dwn_x0,sess2_prev)\n",
        "            logging.debug('process9 - {}' .format(process9.memory_info().rss))  \n",
        "\n",
        "            pred_label0 = not_dwn_labels[class_y0.argmax(axis=1)]\n",
        "            pred_phase0  = not_dwn_phase_labels[phase_y0.argmax(axis=1)]\n",
        "            pred_ground0 = not_dwn_ground_labels[phase_y0.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label0)\n",
        "            predictions.append(pred_phase0)\n",
        "            predictions.append(pred_ground0)\n",
        "            predictions.append(twacs_y0)\n",
        "            predictions.append(pos_y0)\n",
        "            \n",
        "            # return predictions\n",
        "        \n",
        "        if pos_y1 == 0:\n",
        "            process6 = psutil.Process(os.getpid())\n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[4]+\".onnx\")\n",
        "            dwn_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "            print('new')\n",
        "            # print(dwn_x1)\n",
        "            dwn_x1 = dwn_x1.reshape((1, dwn_x1.shape[0], dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[5]+\".onnx\") \n",
        "            class_y1= extract_tensor(dwn_x1, sess2_new)\n",
        "            logging.debug('process6 - {}' .format(process6.memory_info().rss)) \n",
        "                \n",
        "            # intermediate operations to predict phase\n",
        "            process7 = psutil.Process(os.getpid())\n",
        "            indy1 = np.argmax(class_y1)\n",
        "            class_y1 = np.zeros(class_y1.shape)\n",
        "            class_y1[0, indy1] = 1\n",
        "            \n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[6]+\".onnx\")\n",
        "            phase_dwn_x1 = pyt_predict_phase([sliced_x,class_y1],sesson=sess1_new)\n",
        "            phase_dwn_x1 = np.hstack((np.tile(class_y1, (phase_dwn_x1.shape[0], 1)), phase_dwn_x1))\n",
        "            phase_dwn_x1 = phase_dwn_x1.reshape((1,phase_dwn_x1.shape[0],phase_dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[7]+\".onnx\")\n",
        "            phase_y1 = extract_tensor(phase_dwn_x1,sess2_new)\n",
        "            logging.debug('process7 - {}' .format(process7.memory_info().rss))  \n",
        "            pred_label1 = dwn_labels[class_y1.argmax(axis=1)]\n",
        "            pred_phase1 = dwn_phase_labels[phase_y1.argmax(axis=1)]\n",
        "            pred_ground1 = dwn_ground_labels[phase_y1.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label1)\n",
        "            predictions.append(pred_phase1)\n",
        "            predictions.append(pred_ground1)\n",
        "            predictions.append(twacs_y1)\n",
        "            predictions.append(pos_y1)\n",
        "            # return predictions\n",
        "        elif pos_y1==1:\n",
        "            process8 = psutil.Process(os.getpid())\n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[8]+\".onnx\")\n",
        "            not_dwn_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "            # print(not_dwn_x1)\n",
        "            not_dwn_x1 = not_dwn_x1.reshape((1, not_dwn_x1.shape[0], not_dwn_x1.shape[1]))  \n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[9]+\".onnx\")\n",
        "            \n",
        "            class_y1= extract_tensor(not_dwn_x1, sess2_new)\n",
        "            logging.debug('process8 - {}' .format(process8.memory_info().rss))\n",
        "\n",
        "            # intermediate operations to predict phase\n",
        "            process9 = psutil.Process(os.getpid())\n",
        "            sliced_x[:,:, mask_1] = 0.0\n",
        "            sliced_x[:,:, mask_2] = 0.0 \n",
        "            indy1 = np.argmax(class_y1)\n",
        "            class_y1 = np.zeros(class_y1.shape)\n",
        "            class_y1[0, indy1] = 1\n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[10]+\".onnx\")\n",
        "            phase_not_dwn_x1 = pyt_predict_phase(input_name=[sliced_x,class_y1],sesson=sess2_new)\n",
        "            phase_not_dwn_x1 = np.hstack((np.tile(class_y1, (phase_not_dwn_x1.shape[0], 1)), phase_not_dwn_x1))\n",
        "            phase_not_dwn_x1 = phase_not_dwn_x1.reshape((1,phase_not_dwn_x1.shape[0], phase_not_dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[11]+\".onnx\")\n",
        "            phase_y1 = extract_tensor(phase_not_dwn_x1,sess2_new)\n",
        "            logging.debug('process9 - {}' .format(process9.memory_info().rss))\n",
        "            pred_label1 = not_dwn_labels[class_y1.argmax(axis=1)]\n",
        "            pred_phase1  = not_dwn_phase_labels[phase_y1.argmax(axis=1)]\n",
        "            pred_ground1 = not_dwn_ground_labels[phase_y1.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label1)\n",
        "            predictions.append(pred_phase1)\n",
        "            predictions.append(pred_ground1)\n",
        "            predictions.append(twacs_y1)\n",
        "            predictions.append(pos_y1)\n",
        "        return predictions\n",
        "    # return pred_label,pred_phase,pred_ground,twacs_y,pos_y\n",
        "if __name__=='__main__':\n",
        "    final_process = psutil.Process(os.getpid())\n",
        "    predictions = predict_label()\n",
        "    print(predictions)\n",
        "    pred_label0 = predictions[0]\n",
        "    pred_phase0 = predictions[1]\n",
        "    pred_ground0 = predictions[2]\n",
        "    twacs_y0 = predictions[3]\n",
        "    pos_y0 =predictions[4]\n",
        "\n",
        "    pred_label1 = predictions[5]\n",
        "    pred_phase1 =predictions[6]\n",
        "    pred_ground1 = predictions[7]\n",
        "    twacs_y1 = predictions[8]\n",
        "    pos_y1 = predictions[9]  \n",
        "\n",
        "    pred_label0  = pred_label0[0][:-1]\n",
        "    print('pred_label prev ',pred_label0)\n",
        "    logging.debug('pred_label - {}' .format(pred_label0))\n",
        "    pred_phase0  = pred_phase0[0]\n",
        "    print('pred_phase prev',pred_phase0)\n",
        "    logging.debug('pred_phase - {}' .format(pred_phase0))\n",
        "    pred_ground0 = pred_ground0[0]\n",
        "    print('pred_ground prev',pred_ground0)\n",
        "    logging.debug('pred_ground - {}' .format(pred_ground0))\n",
        "    pred_twac0   = twacs_y0\n",
        "    print('pred_twac prev',pred_twac0)\n",
        "    logging.debug('pred_twac - {}' .format(pred_twac0))\n",
        "    pred_pos0    = pos_y0\n",
        "    print('pred_pos prev',pred_pos0)\n",
        "    logging.debug('pred_pos - {}' .format(pred_pos0))\n",
        "    print('***************** New Model Predictions **********************')\n",
        "    pred_label1  = pred_label1[0][:-1]\n",
        "    print('pred_label new',pred_label1)\n",
        "    logging.debug('pred_label - {}' .format(pred_label1))\n",
        "    pred_phase1  = pred_phase1[0]\n",
        "    print('pred_phase new ',pred_phase1)\n",
        "    logging.debug('pred_phase - {}' .format(pred_phase1))\n",
        "    pred_ground1 = pred_ground1[0]\n",
        "    print('pred_ground new',pred_ground1)\n",
        "    logging.debug('pred_ground - {}' .format(pred_ground1))\n",
        "    pred_twac1   = twacs_y1\n",
        "    print('pred_twac new',pred_twac1)\n",
        "    logging.debug('pred_twac - {}' .format(pred_twac1))\n",
        "    pred_pos1    = pos_y1\n",
        "    print('pred_pos new',pred_pos1)\n",
        "    logging.debug('pred_pos - {}' .format(pred_pos1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4ulA9gClqw4"
      },
      "source": [
        "# SQL file test for march2023 model mismatch \n",
        "## To be Deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m7mZtnlmvdr"
      },
      "outputs": [],
      "source": [
        "sql_raw_data2 = np.load('/content/single_notdown_full_raw_mar_530.npy',allow_pickle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTKGsSGFpf4a",
        "outputId": "37a15bea-1e5b-487d-ad9d-ce08a5cda95d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14994.886 14994.359 14995.031 ... 25903.135 25899.516 25896.53 ]\n",
            "Data shape : (30600,)\n",
            "scaled (31, 600)\n",
            "bias (9, 120)\n",
            "inside nan\n",
            "bias x0 [0.07482023 0.66       0.12098088 0.66       0.40606281 1.\n",
            " 1.         0.66       1.73304844]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-841662254daa>:635: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  scaled_x,bias_x,raw_x = np.array(normalize(rms_vec=rms_raw_vecs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03724968433380127\n",
            "0.00042062997817993164\n",
            "pred_label prev  15110\n",
            "pred_phase prev 1\n",
            "pred_ground prev 0\n",
            "pred_twac prev 0\n",
            "pred_pos prev 0\n",
            "***************** New Model Predictions **********************\n",
            "pred_label new 15110\n",
            "pred_phase new  1\n",
            "pred_ground new 0\n",
            "pred_twac new 0\n",
            "pred_pos new 0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import  glob\n",
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "import pickle\n",
        "import sys\n",
        "import datetime\n",
        "import uuid\n",
        "import os\n",
        "import subprocess\n",
        "import os, psutil\n",
        "import logging\n",
        "import onnxruntime as ort\n",
        "\n",
        "# logging.basicConfig(filename = '/content/dfa_ml.log',level=logging.DEBUG)#,encoding = 'utf-8'\n",
        "\n",
        "logging.basicConfig(filename = '/Users/apple/Documents/work_DFA/deployment/dfa_ml.log',level=logging.DEBUG)#,encoding = 'utf-8'\n",
        "name = '/content/113001001004_20230324_010552.pqd.bin'\n",
        "\n",
        "as_strided = np.lib.stride_tricks.as_strided\n",
        "mask_1 = np.array(range(4,9))\n",
        "mask_2 = np.array(range(13,37))\n",
        "\n",
        "user_ids = [1000008,1000009]\n",
        "prev_version_models= ['ext_pos','tree_model_pos',\n",
        "                      'ext_twacs','model_twacs',\n",
        "                      'ext_dwn','model_dwn',\n",
        "                      'ext_phase_dwn','model_phase_dwn',\n",
        "                      'ext_not_dwn','model_not_dwn',\n",
        "                      'ext_phase_not_dwn','model_phase_not_dwn']\n",
        "current_version_models = ['pos_extracted_model','pos_gru_model',\n",
        "                      'twacs_extracted_model','twacs_gru_model',\n",
        "                      'downstream_extracted_model_v2','down_gru_model_v2',\n",
        "                      'downstream_phase_extracted_model','down_phase_gru_model',\n",
        "                      'not_downstream_extracted_model_v2','not_down_gru_model_v2',\n",
        "                      'not_downstream_phase_extracted_model','not_down_phase_gru_model']\n",
        "bin_location = '/dfa/waveforms/'\n",
        "## data ingest\n",
        "# name = sys.argv[1]\n",
        "# waveform_id = sys.argv[3]\n",
        "# monitor_id = sys.argv[2]\n",
        "# row_id = sys.argv[4]\n",
        "# guid = uuid.uuid4()\n",
        "\n",
        "file = open(name, \"rb\")\n",
        "data = np.fromfile(file, '<f4') \n",
        "\n",
        "print(data) \n",
        "print('Data shape : '+ str(data.shape))\n",
        "arr_size = data.shape[0]\n",
        "data1 = []\n",
        "data1.append(data)\n",
        "\n",
        "down = [0,1]\n",
        "\n",
        "# file_pat =  '*steps_50offset.npy'\n",
        "dwn_labels = np.array( ['101100', '101200', '121100', '131100', '131200', '131210',\n",
        "                        '131700', '151100', '201300', '201400', '202100', '205100',\n",
        "                        '205200', '231000', '231900', '233100', '302000'])\n",
        "not_dwn_labels = np.array(['101101', '101201', '121101',\n",
        "                           '131701', '132101', '132201', \n",
        "                           '133101', '151101', '202101'] )\n",
        "#[A B C AB BC CA ABC AB BC CA ABC]\n",
        "dwn_phase_labels     = np.array([0, 1, 2, 3, 4, 5, 6, 3, 4, 5, 6])\n",
        "#[A B C AB BC CA ABC ABN]\n",
        "not_dwn_phase_labels = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "#[AN BN CN AB BC CA ABC ABN BCN CAN ABCN]\n",
        "dwn_ground_labels     = np.array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])\n",
        "# Ground is always unk for non downstream\n",
        "not_dwn_ground_labels = np.array([3, 3, 3, 3, 3, 3, 3])\n",
        "\n",
        "class Indx(IntEnum):\n",
        "    ID = 0\n",
        "    MONITOR = 1\n",
        "    WAVEFORM = 2\n",
        "    FILE = 3\n",
        "    CLASS = 4\n",
        "    POSITION = 5\n",
        "    PHASE = 6\n",
        "    GROUND = 7\n",
        "    FREQ = 8\n",
        "    SECONDS = 9\n",
        "    FLAG = 10\n",
        "    DELTA_PT = 11\n",
        "    THREE_WIRE = 12\n",
        "    BLOB = 13\n",
        "    RANDOM = 14\n",
        "    LAST_CHANNEL = 31\n",
        "class SIndx(IntEnum):\n",
        "    ID = 0\n",
        "    NAME = 1\n",
        "    PT_S_A = 2\n",
        "    PT_S_B = 3\n",
        "    PT_S_C = 4\n",
        "    PT_P_A = 5\n",
        "    PT_P_B = 6\n",
        "    PT_P_C = 7\n",
        "    CT_S_A = 8\n",
        "    CT_S_B = 9\n",
        "    CT_S_C = 10\n",
        "    CT_P_A = 11\n",
        "    CT_P_B = 12\n",
        "    CT_P_C = 13\n",
        "    VA_SCALE = 14\n",
        "    VB_SCALE = 15\n",
        "    VC_SCALE = 16\n",
        "    IA_SCALE = 17\n",
        "    IB_SCALE = 18\n",
        "    IC_SCALE = 19\n",
        "    PQ_SCALE = 99\n",
        "    NO_SCALE = 100\n",
        "class Channels(IntEnum):\n",
        "    VA    =    0\n",
        "    VB    =    1\n",
        "    VC    =    2\n",
        "    IA    =    3\n",
        "    IB    =    4\n",
        "    IC    =    5\n",
        "    IN    =    6\n",
        "    VA_PD =    7\n",
        "    VB_PD =    8\n",
        "    VC_PD =    9\n",
        "    IA_PD =    10\n",
        "    IB_PD =    11\n",
        "    IC_PD =    12\n",
        "    IN_PD =    13\n",
        "    PA =       14\n",
        "    PB =       15\n",
        "    PC =       16\n",
        "    QA =       17\n",
        "    QB =       18\n",
        "    QC =       19\n",
        "    VA_SD =    20\n",
        "    VB_SD =    21\n",
        "    VC_SD =    22\n",
        "    IA_SD =    23\n",
        "    IB_SD =    24\n",
        "    IC_SD =    25\n",
        "    IN_SD =    26\n",
        "    VA_EVENS = 27\n",
        "    VB_EVENS = 28\n",
        "    VC_EVENS = 29\n",
        "    IA_EVENS = 30\n",
        "    IB_EVENS = 31\n",
        "    IC_EVENS = 32\n",
        "    IN_EVENS = 33\n",
        "    VA_ODDS =  34\n",
        "    VB_ODDS =  35\n",
        "    VC_ODDS =  36\n",
        "    IA_ODDS =  37\n",
        "    IB_ODDS =  38\n",
        "    IC_ODDS =  39\n",
        "    IN_ODDS =  40\n",
        "    \n",
        "    VA_NONS = 41\n",
        "    VB_NONS = 42\n",
        "    VC_NONS = 43\n",
        "    IA_NONS = 44\n",
        "    IB_NONS = 45\n",
        "    IC_NONS = 46\n",
        "    IN_NONS = 47\n",
        "    VAB = 48\n",
        "    VBC = 49\n",
        "    VCA = 50\n",
        "    \n",
        "class ce(IntEnum):\n",
        "    v = 0\n",
        "    i = 3\n",
        "    v_pd = 6\n",
        "    i_pd = 9\n",
        "    v_sd = 12\n",
        "    i_sd = 15\n",
        "    v_evens = 18\n",
        "    i_evens = 21\n",
        "    v_odds = 24\n",
        "    i_odds = 27\n",
        "    p = 30\n",
        "    q = 33\n",
        "\n",
        "class used_chnls(IntEnum):\n",
        "    VA    =    0\n",
        "    VB    =    1\n",
        "    VC    =    2\n",
        "    IA    =    3\n",
        "    IB    =    4\n",
        "    IC    =    5\n",
        "    IN    =    6\n",
        "    VA_PD =    7\n",
        "    VB_PD =    8\n",
        "    VC_PD =    9\n",
        "    IA_PD =    10\n",
        "    IB_PD =    11\n",
        "    IC_PD =    12\n",
        "    IN_PD =    13\n",
        "    IA_EVENS = 14\n",
        "    IB_EVENS = 15\n",
        "    IC_EVENS = 16\n",
        "    IN_EVENS = 17\n",
        "    PA =       18\n",
        "    PB =       19\n",
        "    PC =       20\n",
        "    QA =       21\n",
        "    QB =       22\n",
        "    QC =       23\n",
        "    IA_NONS = 24\n",
        "    IB_NONS = 25\n",
        "    IC_NONS = 26\n",
        "    IN_NONS = 27\n",
        "    VAB = 28\n",
        "    VBC = 29\n",
        "    VCA = 30\n",
        "\n",
        "def get_raw_vector_bulk(records_in, channel):\n",
        "    arr = np.frombuffer(records_in[-1], dtype=np.float32)\n",
        "    leng = int(arr_size/51)#arr[Indx.SECONDS] * arr[Indx.FREQ]\n",
        "    # array size constant 660 #######change\n",
        "    vect_size  = arr[(leng*channel):(leng*(channel + 1))]\n",
        "    vect = vect_size#[start:end]\n",
        "    return vect\n",
        "\n",
        "def all_channel_input(input_data = data1):\n",
        "    new_result = dict()\n",
        "    channels = ['VA','VB','VC',\n",
        "                'IA','IB','IC','IN',\n",
        "                'VA_PD','VB_PD','VC_PD',\n",
        "                'IA_PD','IB_PD','IC_PD','IN_PD',\n",
        "                'IA_EVENS','IB_EVENS','IC_EVENS','IN_EVENS',\n",
        "                'PA','PB','PC',\n",
        "                'QA','QB','QC',\n",
        "                'IA_NONS','IB_NONS','IC_NONS','IN_NONS',\n",
        "                'VAB','VBC','VCA'           \n",
        "    ]\n",
        "    \n",
        "    rms_raw_vecs = []\n",
        "\n",
        "    for j in channels:\n",
        "        \n",
        "        if j == 'VA':\n",
        "            channel=Channels.VA\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j == 'VB':\n",
        "            channel=Channels.VB\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VC':\n",
        "            channel=Channels.VC\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "        #---\n",
        "        elif j == 'IA':\n",
        "            channel=Channels.IA\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB':\n",
        "            channel=Channels.IB\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC':\n",
        "            channel=Channels.IC\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN':\n",
        "            channel=Channels.IN\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        #---\n",
        "        elif j == 'VA_PD':\n",
        "            channel=Channels.VA_PD\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j == 'VB_PD':\n",
        "            channel=Channels.VB_PD\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VC_PD':\n",
        "            channel=Channels.VC_PD\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "        #---\n",
        "        elif j == 'IA_PD':\n",
        "            channel=Channels.IA_PD\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_PD':\n",
        "            channel=Channels.IB_PD\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_PD':\n",
        "            channel=Channels.IC_PD\n",
        "            scale_indx = SIndx.IC_SCALE    \n",
        "        elif j =='IN_PD':\n",
        "            channel=Channels.IN_PD\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        \n",
        "        #---\n",
        "        elif j == 'IA_EVENS':\n",
        "            channel=Channels.IA_EVENS\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_EVENS':\n",
        "            channel=Channels.IB_EVENS\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_EVENS':\n",
        "            channel=Channels.IC_EVENS\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN_EVENS':\n",
        "            channel=Channels.IN_EVENS\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "            \n",
        "        #---\n",
        "        elif j =='PA':\n",
        "            channel=Channels.PA\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j =='PB':\n",
        "            channel=Channels.PB\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j =='PC':\n",
        "            channel=Channels.PC\n",
        "            scale_indx = SIndx.VC_SCALE                \n",
        "        #---\n",
        "        elif j =='QA':\n",
        "            channel=Channels.QA\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j =='QB':\n",
        "            channel=Channels.QB\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j =='QC':\n",
        "            channel=Channels.QC\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "\n",
        "        #---    \n",
        "        elif j =='IA_NONS':\n",
        "            channel=Channels.IA_NONS\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_NONS':\n",
        "            channel=Channels.IB_NONS\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_NONS':\n",
        "            channel=Channels.IC_NONS\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN_NONS':\n",
        "            channel=Channels.IN_NONS\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        #---\n",
        "        elif j =='VAB':\n",
        "            channel=Channels.VAB\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j =='VBC':\n",
        "            channel=Channels.VBC\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VCA':\n",
        "            channel=Channels.VCA\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "            \n",
        "            \n",
        "                \n",
        "        vect = get_raw_vector_bulk(records_in = data1, channel=channel)\n",
        "        rms_raw_vecs.append(vect)\n",
        "            \n",
        "    return rms_raw_vecs\n",
        "i_params ={\n",
        "    'small': 10,\n",
        "    'med_start': 100,\n",
        "    'med_end': 500,\n",
        "    'big': 1000\n",
        "    }\n",
        "\n",
        "pq_params ={\n",
        "    'small': 10000,\n",
        "    'med_start': 50000,\n",
        "    'med_end': 100000,\n",
        "    'big': 500000\n",
        "    }\n",
        "\n",
        "def fuzzy_scale(val, params):\n",
        "    ret = 0.0\n",
        "    if val <= params['small']:\n",
        "        ret = 0.33\n",
        "    elif val < params['med_start']:\n",
        "        ret  = 0.33 + (val - params['small']) * (0.33/(params['med_start'] - params['small']))\n",
        "    elif val <= params['med_end']:\n",
        "        ret = 0.66\n",
        "    elif val < params['big']:\n",
        "        ret  = 0.66 + (val - params['med_end']) * (0.33/(params['big'] - params['med_end']))\n",
        "    else:\n",
        "        ret = 1.0\n",
        "    return ret\n",
        "\n",
        "############ ends ##############\n",
        "\n",
        "def normalize(rms_vec,new_max = 1):\n",
        "        #change as per requirement, default selected sliced index for test purpose\n",
        "    _all =rms_vec# sql_raw_data[4]#sql_raw_data[4]#raw_data[4]#rms_vec\n",
        "    _all=np.array(_all)\n",
        "    \n",
        "\n",
        "    \n",
        "    ### new normalization ###\n",
        "    ########################################## VA,VB,VC ##########################\n",
        "    vabc_all = np.stack((_all[used_chnls.VA],_all[used_chnls.VB],_all[used_chnls.VC]))\n",
        "    \n",
        "    vabc_max = np.max(vabc_all)\n",
        "    vabc_min = np.min(vabc_all)\n",
        "    v_range = vabc_max - vabc_min\n",
        "    if new_max == 0:\n",
        "        if v_range < 1e-10 or vabc_max < 1:\n",
        "            print('found vabc_max ' + str(v_range)+' <0 or '+ str(vabc_max)+' <0' )\n",
        "            norm_va,norm_vb,norm_vc = ['x'],['x'],['x']\n",
        "            v_bias = ['x']\n",
        "        else:\n",
        "            #skip the record\n",
        "            # vabc_max if <1 skip this\n",
        "            v_scale = v_range/vabc_max\n",
        "            norm_va = (_all[used_chnls.VA] - vabc_min)/(v_range)\n",
        "            norm_vb = (_all[used_chnls.VB] - vabc_min)/(v_range)\n",
        "            norm_vc = (_all[used_chnls.VC] - vabc_min)/(v_range)\n",
        "            v_bias = [v_scale] * 120\n",
        "    else:\n",
        "        if v_range < 1e-10 or vabc_max < 1:\n",
        "            print('found ' + str(v_range)+' <0 or '+ str(vabc_max)+' <0' )\n",
        "            norm_va,norm_vb,norm_vc = ['x'],['x'],['x']\n",
        "            v_bias = ['x']\n",
        "        else:\n",
        "            v_scale = v_range/vabc_max\n",
        "            norm_va = (_all[used_chnls.VA])/(vabc_max)\n",
        "            norm_vb = (_all[used_chnls.VB])/(vabc_max)\n",
        "            norm_vc = (_all[used_chnls.VC])/(vabc_max)\n",
        "            v_bias = [v_scale] * 120\n",
        "    \n",
        "    ########################################## IA,IB,IC ##########################\n",
        "    iabc_all = np.stack((_all[used_chnls.IA],_all[used_chnls.IB],_all[used_chnls.IC],_all[used_chnls.IN]))\n",
        "    \n",
        "    iabc_max = np.max(iabc_all)\n",
        "    iabc_min = np.min(iabc_all)\n",
        "    i_range = iabc_max - iabc_min #skip the record if < 0.001\n",
        "    if new_max == 0:\n",
        "        if i_range < 1e-20 or iabc_max < 1e-10:\n",
        "            print('found i_range' + str(i_range)+' <0.001'  )\n",
        "            norm_ia,norm_ib,norm_ic,norm_in = ['x'],['x'],['x'],['x']\n",
        "            i_bias = ['x']\n",
        "        else:\n",
        "            i_scale = fuzzy_scale(i_range, i_params)\n",
        "            norm_ia = (_all[used_chnls.IA] - iabc_min)/(i_range)\n",
        "            norm_ib = (_all[used_chnls.IB] - iabc_min)/(i_range)\n",
        "            norm_ic = (_all[used_chnls.IC] - iabc_min)/(i_range)\n",
        "            norm_in = (_all[used_chnls.IN] - iabc_min)/(i_range)\n",
        "            i_bias = [i_scale] * 120\n",
        "    else:\n",
        "        if i_range < 1e-20 or iabc_max < 1e-10:\n",
        "            print('found ' + str(i_range)+' <0.001'  )\n",
        "            norm_ia,norm_ib,norm_ic,norm_in = ['x'],['x'],['x'],['x']\n",
        "            i_bias = ['x']\n",
        "        else:\n",
        "            i_scale = fuzzy_scale(i_range, i_params)\n",
        "            norm_ia = (_all[used_chnls.IA])/(iabc_max)\n",
        "            norm_ib = (_all[used_chnls.IB])/(iabc_max)\n",
        "            norm_ic = (_all[used_chnls.IC])/(iabc_max)\n",
        "            norm_in = (_all[used_chnls.IN])/(iabc_max)\n",
        "            i_bias = [i_scale]* 120\n",
        "            \n",
        "    \n",
        "    \n",
        "    #################################### 'VA_PD','VB_PD','VC_PD'  #############################\n",
        "    PD_vabc_all =  np.stack((_all[used_chnls.VA_PD],_all[used_chnls.VB_PD],_all[used_chnls.VC_PD]))\n",
        "    \n",
        "    PD_vabc_max = np.max(PD_vabc_all)\n",
        "    PD_vabc_min = np.min(PD_vabc_all)\n",
        "    v_pd_scale = PD_vabc_max/vabc_max #vabc_max<1 skip and PD_vabc_max< 1\n",
        "    if PD_vabc_max < 1e-10:\n",
        "        print('found PD_vabc_max',str(vabc_max),str(PD_vabc_max), ' < 1'  )\n",
        "        norm_va_pd,norm_vb_pd,norm_vc_pd = ['x'],['x'],['x']\n",
        "        vpd_bias = ['x'] \n",
        "    else: \n",
        "        norm_va_pd = (_all[used_chnls.VA_PD] )/( PD_vabc_max)\n",
        "        norm_vb_pd = (_all[used_chnls.VB_PD] )/( PD_vabc_max)\n",
        "        norm_vc_pd = (_all[used_chnls.VC_PD] )/( PD_vabc_max)\n",
        "        vpd_bias = [v_pd_scale]* 120\n",
        "    \n",
        "    ################################  IA_PD,IB_PD,IC_PD,IN_PD  #############################\n",
        "    PD_iabc_all = np.stack((_all[used_chnls.IA_PD],_all[used_chnls.IB_PD],_all[used_chnls.IC_PD],_all[used_chnls.IN_PD]))\n",
        "    \n",
        "    PD_iabc_max = np.max(PD_iabc_all)\n",
        "    PD_iabc_min = np.min(PD_iabc_all)\n",
        "    if PD_iabc_max<1e-10:\n",
        "        print('found PD_iabc_max ' + str(PD_iabc_max)+' <0.001'  )\n",
        "        norm_ia_pd,norm_ib_pd,norm_ic_pd,norm_ic_pd = ['x'],['x'],['x'],['x']\n",
        "        ipd_bias = ['x'] \n",
        "    else:\n",
        "        i_pd_scale = fuzzy_scale(PD_iabc_max, i_params)\n",
        "        norm_ia_pd = (_all[used_chnls.IA_PD] )/(PD_iabc_max)# PD_iabc_max <0.001\n",
        "        norm_ib_pd = (_all[used_chnls.IB_PD])/(PD_iabc_max)\n",
        "        norm_ic_pd = (_all[used_chnls.IC_PD])/(PD_iabc_max)\n",
        "        norm_in_pd = (_all[used_chnls.IN_PD])/(PD_iabc_max)\n",
        "        ipd_bias = [i_pd_scale]* 120\n",
        "    \n",
        "    ################################# I_EVEN & I_ODD  ####################################\n",
        "    ## I_EVEN\n",
        "    even_iabc_all = np.stack((_all[used_chnls.IA_EVENS],_all[used_chnls.IB_EVENS],_all[used_chnls.IC_EVENS],_all[used_chnls.IN_EVENS]))\n",
        "    \n",
        "    even_iabc_max = np.max(even_iabc_all)\n",
        "    even_iabc_min = np.min(even_iabc_all)\n",
        "    if even_iabc_max<1e-10:\n",
        "        print('found even_iabc_max ' + str(even_iabc_all)+' <0.e-10'  )\n",
        "        norm_ia_even,norm_ib_even,norm_ic_even,norm_in_even = ['x'],['x'],['x'],['x']\n",
        "        ieven_bias = ['x'] \n",
        "    else:\n",
        "        i_even_scale = fuzzy_scale(even_iabc_max, i_params)\n",
        "        norm_ia_even = (_all[used_chnls.IA_EVENS] )/(even_iabc_max)# even_iabc_max <0.001\n",
        "        norm_ib_even = (_all[used_chnls.IB_EVENS])/(even_iabc_max)\n",
        "        norm_ic_even = (_all[used_chnls.IC_EVENS])/(even_iabc_max)\n",
        "        norm_in_even = (_all[used_chnls.IN_EVENS])/(even_iabc_max)\n",
        "        ieven_bias = [i_even_scale] * 120\n",
        "        \n",
        "    #######################################   PA,PB,PC  ##################################\n",
        "    pabc_all = np.stack((_all[used_chnls.PA],_all[used_chnls.PB],_all[used_chnls.PC]))\n",
        "    \n",
        "    pabc_min = np.min(pabc_all)\n",
        "    pabc_max = np.max(pabc_all)\n",
        "    p_range = pabc_max - pabc_min # p_range<1 skip\n",
        "    if p_range<0.001:\n",
        "        print('found p_range' + str(p_range)+' <1'  )\n",
        "        norm_pa,norm_pb,norm_pc = ['x'],['x'],['x']\n",
        "        p_bias = ['x']\n",
        "    else:\n",
        "        p_scale = fuzzy_scale(p_range, pq_params)\n",
        "        norm_pa = (_all[used_chnls.PA] - float(pabc_min))/p_range\n",
        "        norm_pb = (_all[used_chnls.PB] - float(pabc_min))/p_range\n",
        "        norm_pc = (_all[used_chnls.PC] - float(pabc_min))/p_range\n",
        "        p_bias = [p_scale] * 120\n",
        "\n",
        "    # QA,QB,QC\n",
        "    qabc_all = np.stack((_all[used_chnls.QA],_all[used_chnls.QB],_all[used_chnls.QC]))\n",
        "    qabc_min = np.min(qabc_all)\n",
        "    qabc_max = np.max(qabc_all)\n",
        "    q_range = qabc_max - qabc_min # q_range<1 skip \n",
        "    if q_range< 0.001:\n",
        "        print('found q_range' + str(q_range)+' <1'  )\n",
        "        norm_qa,norm_qb,norm_qc = ['x'],['x'],['x']\n",
        "        q_bias = ['x'] \n",
        "    else:  \n",
        "        q_scale = fuzzy_scale(q_range, pq_params)\n",
        "        norm_qa= (_all[used_chnls.QA] - qabc_min)/q_range\n",
        "        norm_qb= (_all[used_chnls.QB] - qabc_min)/q_range\n",
        "        norm_qc= (_all[used_chnls.QC] - qabc_min)/q_range\n",
        "        q_bias = [q_scale] * 120\n",
        "    \n",
        "    ########################## 'IA_NONS','IB_NONS','IC_NONS','IN_NONS' #########################\n",
        "    nons_iabc_all = np.stack((_all[used_chnls.IA_NONS],_all[used_chnls.IB_NONS],_all[used_chnls.IC_NONS],_all[used_chnls.IN_NONS]))\n",
        "\n",
        "    nons_iabc_max = np.max(nons_iabc_all)\n",
        "    nons_iabc_min = np.min(nons_iabc_all)\n",
        "    if nons_iabc_max<0.001:\n",
        "        print('found ' + str(nons_iabc_max)+' <0.001'  )\n",
        "        norm_ia_nons,norm_ib_nons,norm_ic_nons,norm_in_nons = ['x'],['x'],['x'],['x']\n",
        "        inons_bias = ['x'] \n",
        "    else:\n",
        "        i_nons_scale = fuzzy_scale(nons_iabc_max, i_params)\n",
        "        norm_ia_nons = (_all[used_chnls.IA_NONS] )/(nons_iabc_max)\n",
        "        norm_ib_nons = (_all[used_chnls.IB_NONS])/(nons_iabc_max)\n",
        "        norm_ic_nons = (_all[used_chnls.IC_NONS])/(nons_iabc_max)\n",
        "        norm_in_nons = (_all[used_chnls.IN_NONS])/(nons_iabc_max)\n",
        "        inons_bias = [i_nons_scale] * 120\n",
        "        \n",
        "    ##################################### 'VAB','VBC','VCA'  ###################################\n",
        "    vabc_ll_all =  np.stack((_all[used_chnls.VAB],_all[used_chnls.VBC],_all[used_chnls.VCA]))\n",
        "    vabc_ll_max = np.max(vabc_ll_all)\n",
        "    vabc_ll_min = np.min(vabc_ll_all)\n",
        "    v_ll_scale = vabc_ll_max/vabc_max #skip vabc_ll_max < 1\n",
        "    if vabc_ll_max<1:\n",
        "        print('found vabc_ll_max '+str(vabc_ll_max)+' <1'  )\n",
        "        norm_va_ll,norm_vb_ll,norm_vc_ll = ['x'],['x'],['x']\n",
        "        vll_bias = ['x'] \n",
        "    else: \n",
        "        norm_va_ll = (_all[used_chnls.VAB] )/( vabc_ll_max)\n",
        "        norm_vb_ll = (_all[used_chnls.VBC] )/( vabc_ll_max)\n",
        "        norm_vc_ll = (_all[used_chnls.VCA] )/( vabc_ll_max)\n",
        "        vll_bias = [v_ll_scale] * 120\n",
        "\n",
        "    ################################## ARRAY STACK ####################################\n",
        "    \n",
        "    bias_list = np.array([np.array(v_bias),np.array(i_bias),\n",
        "                     np.array(vpd_bias) ,np.array(ipd_bias) ,np.array(ieven_bias) ,\n",
        "                     np.array(p_bias),np.array(q_bias) ,\n",
        "                     np.array(inons_bias),np.array(vll_bias)])\n",
        "        \n",
        "        \n",
        "    final_norm = np.array([ norm_va,norm_vb,norm_vc,# np.array(v_bias)\n",
        "                       norm_ia,norm_ib,norm_ic, norm_in,#np.array(i_bias)\n",
        "                       norm_va_pd,norm_vb_pd,norm_vc_pd,# np.array(vpd_bias)\n",
        "                       norm_ia_pd,norm_ib_pd,norm_ic_pd,norm_in_pd, #np.array(ipd_bias)\n",
        "                       norm_ia_even,norm_ib_even,norm_ic_even,norm_in_even,#np.array(ieven_bias)\n",
        "                       norm_pa,norm_pb,norm_pc,# np.array(p_bias)\n",
        "                       norm_qa,norm_qb,norm_qc,#  np.array(q_bias)\n",
        "                       norm_ia_nons,norm_ib_nons,norm_ic_nons,norm_in_nons,\n",
        "                       norm_va_ll,norm_vb_ll,norm_vc_ll\n",
        "                    ])\n",
        "    final_raw_full = np.array([ _all[used_chnls.VA],_all[used_chnls.VB],_all[used_chnls.VC],\n",
        "                     _all[used_chnls.IA],_all[used_chnls.IB],_all[used_chnls.IC],_all[used_chnls.IN],\n",
        "                       _all[used_chnls.VA_PD],_all[used_chnls.VB_PD],_all[used_chnls.VC_PD],\n",
        "                         _all[used_chnls.IA_PD],_all[used_chnls.IB_PD],_all[used_chnls.IC_PD],_all[used_chnls.IN_PD],\n",
        "                             _all[used_chnls.IA_EVENS],_all[used_chnls.IB_EVENS],_all[used_chnls.IC_EVENS],_all[used_chnls.IN_EVENS],\n",
        "                                _all[used_chnls.PA],_all[used_chnls.PB],_all[used_chnls.PC],\n",
        "                                     _all[used_chnls.QA],_all[used_chnls.QB],_all[used_chnls.QC],\n",
        "                                         _all[used_chnls.IA_NONS],_all[used_chnls.IB_NONS],_all[used_chnls.IC_NONS],_all[used_chnls.IN_NONS],\n",
        "                                             _all[used_chnls.VAB],_all[used_chnls.VBC],_all[used_chnls.VCA]                               \n",
        "        \n",
        "                    ])\n",
        "    # print(final_norm.shape)\n",
        "    # print(final_norm[0])\n",
        "    return final_norm, bias_list,final_raw_full\n",
        "\n",
        "def extract_tensor(data,sesson):\n",
        "    sess = sesson\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    output_name = sess.get_outputs()[0].name\n",
        "    res = sess.run([output_name], {input_name: data.astype(np.float32)})\n",
        "    return res[0]\n",
        "\n",
        "def pyt_predict_phase(input_name,sesson):\n",
        "    sess = sesson\n",
        "    input1 = input_name\n",
        "    input_name1 = sess.get_inputs()[0].name\n",
        "    input_name2 = sess.get_inputs()[1].name\n",
        "    output_name = sess.get_outputs()[0].name\n",
        "    res = sess.run([output_name],{input_name1:input1[1].astype(np.float32),input_name2:input1[0].astype(np.float32)})\n",
        "    return res[0]\n",
        "\n",
        "def predict_label():\n",
        "    predictions = []\n",
        "    \n",
        "    user_id0 = user_ids[0]\n",
        "    root_model_path0= '/content/drive/MyDrive/capstone/Feb2023_deployment/version1/'\n",
        "    version0=prev_version_models\n",
        "    \n",
        "    user_id1 = user_ids[1]\n",
        "    root_model_path1= '/content/drive/MyDrive/capstone/Feb2023_deployment/version2/'\n",
        "    version1=current_version_models\n",
        "    \n",
        "    process1 = psutil.Process(os.getpid())\n",
        "    vect = get_raw_vector_bulk(data1,channel = Channels.VA) \n",
        "    logging.debug('process1 - {}' .format(process1.memory_info().rss)) \n",
        "\n",
        "    process2 = psutil.Process(os.getpid())\n",
        "    rms_raw_vecs = all_channel_input(data1)\n",
        "    logging.debug('process2 - {}' .format(process2.memory_info().rss))\n",
        "\n",
        "    process3 = psutil.Process(os.getpid())\n",
        "    scaled_x,bias_x,raw_x = np.array(normalize(rms_vec=rms_raw_vecs))\n",
        "    # scaled_x = file_data[4]\n",
        "    print('scaled', scaled_x.shape)\n",
        "    print('bias', bias_x.shape)\n",
        "    logging.debug('process3 - {}' .format(process3.memory_info().rss)) \n",
        "    # Check for nans\n",
        "    is_nan = len(np.argwhere(np.isnan(bias_x)))\n",
        "    if is_nan == 0:\n",
        "        for xx in scaled_x :\n",
        "            is_nan = len(np.argwhere(np.isnan(xx))) \n",
        "            if is_nan > 0:\n",
        "                break\n",
        "\n",
        "    # Proceed on no nans\n",
        "    if is_nan == 0:\n",
        "        print('inside nan')\n",
        "      # Adding the multi step embedded features needed for LSTM\n",
        "        len_x = 120 * (len(scaled_x[0]) // 120)\n",
        "        print('bias x0', bias_x[:,0])\n",
        "        #Compute number of possible time steps\n",
        "        starts = np.array(range(0,len_x - 120 + 1, 50))\n",
        "        ts = len(starts)     \n",
        "        bias = np.repeat(np.reshape((bias_x[:,0]), (9,1)), len_x, axis=1 )\n",
        "        # print(bias)\n",
        "\n",
        "        main_list = []\n",
        "        for start in starts:\n",
        "            end = int(start)+120\n",
        "            final_norm = np.vstack((bias_x[0], scaled_x[0][start:end],scaled_x[1][start:end],scaled_x[2][start:end],# np.array(v_bias_x)\n",
        "                          bias_x[1] , scaled_x[3][start:end],scaled_x[4][start:end],scaled_x[5][start:end], scaled_x[6][start:end],#np.array(i_bias_x)\n",
        "                            bias_x[2]  , scaled_x[7][start:end],scaled_x[8][start:end],scaled_x[9][start:end],# np.array(vpd_bias_x)\n",
        "                              bias_x[3] , scaled_x[10][start:end],scaled_x[11][start:end],scaled_x[12][start:end],scaled_x[13][start:end], #np.array(ipd_bias_x)\n",
        "                                  bias_x[4] , scaled_x[14][start:end],scaled_x[15][start:end],scaled_x[16][start:end],scaled_x[17][start:end],#np.array(ieven_bias_x)\n",
        "                                          bias_x[5] , scaled_x[18][start:end],scaled_x[19][start:end],scaled_x[20][start:end],# np.array(p_bias_x)\n",
        "                                            bias_x[6] ,  scaled_x[21][start:end],scaled_x[22][start:end],scaled_x[23][start:end],#  np.array(q_bias_x)\n",
        "                                                bias_x[7], scaled_x[24][start:end],scaled_x[25][start:end],scaled_x[26][start:end],scaled_x[27][start:end],\n",
        "                                                  bias_x[8], scaled_x[28][start:end],scaled_x[29][start:end],scaled_x[30][start:end])\n",
        "            ).T\n",
        "            main_list.append(final_norm)\n",
        "        sliced_x = np.array(main_list)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # sliced_x = np.insert(np.stack(scaled_x).astype('float64'),\n",
        "        #                 [0, 3, 7, 10, 14, 18, 21, 24, 28 ], \n",
        "        #                 bias,\n",
        "        #               axis = 0).T\n",
        "        # sliced_x = as_strided(sliced_x, \n",
        "        #                       shape = (ts, 120, 40), \n",
        "        #                       strides = (sliced_x.itemsize*50, sliced_x.strides[0], sliced_x.strides[1]))\n",
        "        # print(sliced_x[0,:,5])\n",
        "\n",
        "\n",
        "\n",
        "        run_slice = sliced_x\n",
        "        np.save('runtime_scliced_x.npy',sliced_x)\n",
        "        # main_list.append(final_norm)\n",
        "        # sliced_x = np.array(main_list)\n",
        "        # Determine postion\n",
        "        process4 = psutil.Process(os.getpid())\n",
        "        \n",
        "        sess1_prev = ort.InferenceSession(root_model_path0 + version0[0]+\".onnx\")\n",
        "        sess1_new = ort.InferenceSession(root_model_path1 + version1[0]+\".onnx\")\n",
        "        \n",
        "        pos_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "        pos_x0 = pos_x0.reshape((1, pos_x0.shape[0], pos_x0.shape[1]))\n",
        "        pos_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "        pos_x1 = pos_x1.reshape((1, pos_x1.shape[0], pos_x1.shape[1]))\n",
        "        \n",
        "        sess2_prev = ort.InferenceSession(root_model_path0 + version0[1]+\".onnx\")\n",
        "        sess2_new = ort.InferenceSession(root_model_path1 + version1[1]+\".onnx\")\n",
        "        pos_y0 = extract_tensor(pos_x0,sess2_prev)\n",
        "        pos_y0 = pos_y0.tolist()\n",
        "        pos_y0 = pos_y0[0][0]\n",
        "        print(pos_y0)\n",
        "        pos_y0 = 0 if pos_y0 < 0.5 else 1 \n",
        "        \n",
        "        \n",
        "        pos_y1 = extract_tensor(pos_x1,sess2_new)\n",
        "        pos_y1 = pos_y1.tolist()\n",
        "        pos_y1 = pos_y1[0][0]\n",
        "        print(pos_y1)\n",
        "        pos_y1 = 0 if pos_y1 < 0.5 else 1 \n",
        "        \n",
        "        logging.debug('process4 - {}' .format(process4.memory_info().rss)) \n",
        "\n",
        "        process5 = psutil.Process(os.getpid())\n",
        "        sess1_prev = ort.InferenceSession(root_model_path0 + version0[2]+\".onnx\")\n",
        "        sess1_new = ort.InferenceSession(root_model_path1 + version1[2]+\".onnx\")\n",
        "        \n",
        "        twacs_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "        twacs_x0 = twacs_x0.reshape((1, twacs_x0.shape[0], twacs_x0.shape[1])) \n",
        "        twacs_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "        twacs_x1 = twacs_x1.reshape((1, twacs_x1.shape[0], twacs_x1.shape[1])) \n",
        "        \n",
        "        sess2_prev = ort.InferenceSession(root_model_path0 + version0[3]+\".onnx\")\n",
        "        sess2_new = ort.InferenceSession(root_model_path1 + version1[3]+\".onnx\")\n",
        "        twacs_y0 = extract_tensor(twacs_x0, sess2_prev)\n",
        "        twacs_y0 = 0 if pos_y0 < 0.7 else 1\n",
        "        twacs_y1 = extract_tensor(twacs_x1, sess2_new)\n",
        "        twacs_y1 = 0 if pos_y1 < 0.7 else 1\n",
        "        \n",
        "        logging.debug('process5 - {}' .format(process5.memory_info().rss)) \n",
        "\n",
        "\n",
        "        if pos_y0 == 0:\n",
        "\n",
        "            process6 = psutil.Process(os.getpid())\n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[4]+\".onnx\")\n",
        "            \n",
        "            dwn_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "            dwn_x0 = dwn_x0.reshape((1, dwn_x0.shape[0], dwn_x0.shape[1]))\n",
        "            \n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[5]+\".onnx\") \n",
        "            class_y0= extract_tensor(dwn_x0, sess2_prev)\n",
        "            logging.debug('process6 - {}' .format(process6.memory_info().rss)) \n",
        "                \n",
        "            # intermediate operations to predict phase\n",
        "            process7 = psutil.Process(os.getpid())\n",
        "            indy0 = np.argmax(class_y0)\n",
        "            class_y0 = np.zeros(class_y0.shape)\n",
        "            class_y0[0, indy0] = 1\n",
        "            \n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[6]+\".onnx\")\n",
        "            phase_dwn_x0 = pyt_predict_phase([sliced_x,class_y0],sesson=sess1_prev)\n",
        "            phase_dwn_x0 = np.hstack((np.tile(class_y0, (phase_dwn_x0.shape[0], 1)), phase_dwn_x0))\n",
        "            phase_dwn_x0 = phase_dwn_x0.reshape((1,phase_dwn_x0.shape[0],phase_dwn_x0.shape[1]))\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[7]+\".onnx\")\n",
        "            phase_y0 = extract_tensor(phase_dwn_x0,sess2_prev)\n",
        "            \n",
        "            logging.debug('process7 - {}' .format(process7.memory_info().rss))  \n",
        "            \n",
        "            pred_label0 = dwn_labels[class_y0.argmax(axis=1)]\n",
        "            pred_phase0  = dwn_phase_labels[phase_y0.argmax(axis=1)]\n",
        "            pred_ground0 = dwn_ground_labels[phase_y0.argmax(axis=1)]\n",
        "           \n",
        "            predictions.append(pred_label0)\n",
        "            predictions.append(pred_phase0)\n",
        "            predictions.append(pred_ground0)\n",
        "            predictions.append(twacs_y0)\n",
        "            predictions.append(pos_y0)\n",
        "\n",
        "        elif pos_y0==1:\n",
        "            process8 = psutil.Process(os.getpid())\n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[8]+\".onnx\")\n",
        "            \n",
        "            not_dwn_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "            not_dwn_x00 = not_dwn_x0\n",
        "            not_dwn_x0 = not_dwn_x0.reshape((1, not_dwn_x0.shape[0], not_dwn_x0.shape[1]))  \n",
        "            \n",
        "            \n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[9]+\".onnx\")\n",
        "            \n",
        "            class_y0= extract_tensor(not_dwn_x0, sess2_prev)\n",
        "            print(np.argmax(class_y0))\n",
        "            logging.debug('process8 - {}' .format(process8.memory_info().rss))\n",
        "\n",
        "            # intermediate operations to predict phase\n",
        "            process9 = psutil.Process(os.getpid())\n",
        "            sliced_x[:,:, mask_1] = 0.0\n",
        "            sliced_x[:,:, mask_2] = 0.0 \n",
        "            \n",
        "            indy0 = np.argmax(class_y0)\n",
        "            class_y0 = np.zeros(class_y0.shape)\n",
        "            print(class_y0)\n",
        "            class_y0[0, indy0] = 1\n",
        "            print(class_y0)\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[10]+\".onnx\")\n",
        "            phase_not_dwn_x0 = pyt_predict_phase(input_name=[sliced_x,class_y0],sesson=sess2_prev)\n",
        "            phase_not_dwn_x0 = np.hstack((np.tile(class_y0, (phase_not_dwn_x0.shape[0], 1)), phase_not_dwn_x0))\n",
        "            phase_not_dwn_x0 = phase_not_dwn_x0.reshape((1,phase_not_dwn_x0.shape[0], phase_not_dwn_x0.shape[1]))\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[11]+\".onnx\")\n",
        "            phase_y0 = extract_tensor(phase_not_dwn_x0,sess2_prev)\n",
        "            logging.debug('process9 - {}' .format(process9.memory_info().rss))  \n",
        "\n",
        "            pred_label0 = not_dwn_labels[class_y0.argmax(axis=1)]\n",
        "            pred_phase0  = not_dwn_phase_labels[phase_y0.argmax(axis=1)]\n",
        "            pred_ground0 = not_dwn_ground_labels[phase_y0.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label0)\n",
        "            predictions.append(pred_phase0)\n",
        "            predictions.append(pred_ground0)\n",
        "            predictions.append(twacs_y0)\n",
        "            predictions.append(pos_y0)\n",
        " \n",
        "        if pos_y1 == 0:\n",
        "            process6 = psutil.Process(os.getpid())\n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[4]+\".onnx\")\n",
        "            dwn_x1 = extract_tensor(run_slice,sess1_new)\n",
        "            dwn_x111 = dwn_x1\n",
        "            dwn_x1 = dwn_x1.reshape((1, dwn_x1.shape[0], dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[5]+\".onnx\") \n",
        "            class_y1= extract_tensor(dwn_x1, sess2_new)\n",
        "            logging.debug('process6 - {}' .format(process6.memory_info().rss)) \n",
        "                \n",
        "            # intermediate operations to predict phase\n",
        "            process7 = psutil.Process(os.getpid())\n",
        "            indy1 = np.argmax(class_y1)\n",
        "            class_y1 = np.zeros(class_y1.shape)\n",
        "            class_y1[0, indy1] = 1\n",
        "            \n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[6]+\".onnx\")\n",
        "            phase_dwn_x1 = pyt_predict_phase([sliced_x,class_y1],sesson=sess1_new)\n",
        "            phase_dwn_x1 = np.hstack((np.tile(class_y1, (phase_dwn_x1.shape[0], 1)), phase_dwn_x1))\n",
        "            phase_dwn_x1 = phase_dwn_x1.reshape((1,phase_dwn_x1.shape[0],phase_dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[7]+\".onnx\")\n",
        "            phase_y1 = extract_tensor(phase_dwn_x1,sess2_new)\n",
        "            logging.debug('process7 - {}' .format(process7.memory_info().rss))  \n",
        "            pred_label1 = dwn_labels[class_y1.argmax(axis=1)]\n",
        "            pred_phase1 = dwn_phase_labels[phase_y1.argmax(axis=1)]\n",
        "            pred_ground1 = dwn_ground_labels[phase_y1.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label1)\n",
        "            predictions.append(pred_phase1)\n",
        "            predictions.append(pred_ground1)\n",
        "            predictions.append(twacs_y1)\n",
        "            predictions.append(pos_y1)\n",
        "\n",
        "        elif pos_y1==1:\n",
        "\n",
        "            process8 = psutil.Process(os.getpid())\n",
        "            print(root_model_path1 + version1[8]+\".onnx\")\n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[8]+\".onnx\")\n",
        "            not_dwn_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "            np.save('not_dwn_x1.npy',not_dwn_x1)\n",
        "            not_dwn_x1 = not_dwn_x1.reshape((1, not_dwn_x1.shape[0], not_dwn_x1.shape[1]))  \n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[9]+\".onnx\")\n",
        "            class_y1= extract_tensor(not_dwn_x1, sess2_new)\n",
        "            print(np.argmax(class_y1))\n",
        "            logging.debug('process8 - {}' .format(process8.memory_info().rss))\n",
        "\n",
        "            # intermediate operations to predict phase\n",
        "            process9 = psutil.Process(os.getpid())\n",
        "            sliced_x[:,:, mask_1] = 0.0\n",
        "            sliced_x[:,:, mask_2] = 0.0 \n",
        "            indy1 = np.argmax(class_y1)\n",
        "            class_y1 = np.zeros(class_y1.shape)\n",
        "            print(class_y1)\n",
        "            class_y1[0, indy1] = 1\n",
        "            print(class_y1)\n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[10]+\".onnx\")\n",
        "            phase_not_dwn_x1 = pyt_predict_phase(input_name=[sliced_x,class_y1],sesson=sess2_new)\n",
        "            phase_not_dwn_x1 = np.hstack((np.tile(class_y1, (phase_not_dwn_x1.shape[0], 1)), phase_not_dwn_x1))\n",
        "            phase_not_dwn_x1 = phase_not_dwn_x1.reshape((1,phase_not_dwn_x1.shape[0], phase_not_dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[11]+\".onnx\")\n",
        "            phase_y1 = extract_tensor(phase_not_dwn_x1,sess2_new)\n",
        "            logging.debug('process9 - {}' .format(process9.memory_info().rss))\n",
        "            pred_label1 = not_dwn_labels[class_y1.argmax(axis=1)]\n",
        "            pred_phase1  = not_dwn_phase_labels[phase_y1.argmax(axis=1)]\n",
        "            pred_ground1 = not_dwn_ground_labels[phase_y1.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label1)\n",
        "            predictions.append(pred_phase1)\n",
        "            predictions.append(pred_ground1)\n",
        "            predictions.append(twacs_y1)\n",
        "            predictions.append(pos_y1)\n",
        "    return predictions#,scaled_x,bias_x,sliced_x,raw_x,not_dwn_x0,not_dwn_x1,not_dwn_x00,not_dwn_x11,run_slice\n",
        "    # return pred_label,pred_phase,pred_ground,twacs_y,pos_y\n",
        "if __name__=='__main__':\n",
        "    final_process = psutil.Process(os.getpid())\n",
        "    # print(predictions)\n",
        "    predictions= predict_label() #,scaled_x,bias_x,sliced_x,raw_x,not_dwn_x0,not_dwn_x1,not_dwn_x00,not_dwn_x11,run_slice \n",
        "    pred_label0 = predictions[0]\n",
        "    pred_phase0 = predictions[1]\n",
        "    pred_ground0 = predictions[2]\n",
        "    twacs_y0 = predictions[3]\n",
        "    pos_y0 =predictions[4]\n",
        "#### new preds\n",
        "    pred_label1 = predictions[5]\n",
        "    pred_phase1 =predictions[6]\n",
        "    pred_ground1 = predictions[7]\n",
        "    twacs_y1 = predictions[8]\n",
        "    pos_y1 = predictions[9]  \n",
        "    \n",
        "    pred_label0  = pred_label0[0][:-1]\n",
        "    print('pred_label prev ',pred_label0)\n",
        "    logging.debug('pred_label - {}' .format(pred_label0))\n",
        "    pred_phase0  = pred_phase0[0]\n",
        "    print('pred_phase prev',pred_phase0)\n",
        "    logging.debug('pred_phase - {}' .format(pred_phase0))\n",
        "    pred_ground0 = pred_ground0[0]\n",
        "    print('pred_ground prev',pred_ground0)\n",
        "    logging.debug('pred_ground - {}' .format(pred_ground0))\n",
        "    pred_twac0   = twacs_y0\n",
        "    print('pred_twac prev',pred_twac0)\n",
        "    logging.debug('pred_twac - {}' .format(pred_twac0))\n",
        "    pred_pos0    = pos_y0\n",
        "    print('pred_pos prev',pred_pos0)\n",
        "    logging.debug('pred_pos - {}' .format(pred_pos0))\n",
        "    print('***************** New Model Predictions **********************')\n",
        "    pred_label1  = pred_label1[0][:-1]\n",
        "    print('pred_label new',pred_label1)\n",
        "    logging.debug('pred_label - {}' .format(pred_label1))\n",
        "    pred_phase1  = pred_phase1[0]\n",
        "    print('pred_phase new ',pred_phase1)\n",
        "    logging.debug('pred_phase - {}' .format(pred_phase1))\n",
        "    pred_ground1 = pred_ground1[0]\n",
        "    print('pred_ground new',pred_ground1)\n",
        "    logging.debug('pred_ground - {}' .format(pred_ground1))\n",
        "    pred_twac1   = twacs_y1\n",
        "    print('pred_twac new',pred_twac1)\n",
        "    logging.debug('pred_twac - {}' .format(pred_twac1))\n",
        "    pred_pos1    = pos_y1\n",
        "    print('pred_pos new',pred_pos1)\n",
        "    logging.debug('pred_pos - {}' .format(pred_pos1))\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test MARCH2023 deploy"
      ],
      "metadata": {
        "id": "Dvc7imcO7WxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import  glob\n",
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "import pickle\n",
        "import sys\n",
        "import datetime\n",
        "import uuid\n",
        "import os\n",
        "import subprocess\n",
        "import os, psutil\n",
        "import logging\n",
        "import onnxruntime as ort\n",
        "\n",
        "logging.basicConfig(filename = '/dfa/logs/dfa_ml.log',level=logging.DEBUG)#,encoding = 'utf-8'\n",
        "\n",
        "as_strided = np.lib.stride_tricks.as_strided\n",
        "mask_1 = np.array(range(4,9))\n",
        "mask_2 = np.array(range(13,37))\n",
        "\n",
        "user_ids = [1000008,1000009]\n",
        "prev_version_models= ['ext_pos','tree_model_pos',\n",
        "                      'ext_twacs','model_twacs',\n",
        "                      'ext_dwn','model_dwn',\n",
        "                      'ext_phase_dwn','model_phase_dwn',\n",
        "                      'ext_not_dwn','model_not_dwn',\n",
        "                      'ext_phase_not_dwn','model_phase_not_dwn']\n",
        "current_version_models = ['pos_extracted_model','pos_gru_model',\n",
        "                      'twacs_extracted_model','twacs_gru_model',\n",
        "                      'downstream_extracted_model','down_gru_model',\n",
        "                      'downstream_phase_extracted_model','down_phase_gru_model',\n",
        "                      'not_downstream_extracted_model_v2','not_down_gru_model_v2',\n",
        "                      'not_downstream_phase_extracted_model','not_down_phase_gru_model']\n",
        "bin_location = '/dfa/waveforms/'\n",
        "\n",
        "name= '/content/113004001003_20230323_134646.pqd.bin'\n",
        "## data ingest\n",
        "# name = sys.argv[1]\n",
        "# waveform_id = sys.argv[3]\n",
        "# monitor_id = sys.argv[2]\n",
        "# row_id = sys.argv[4]\n",
        "# guid = uuid.uuid4()\n",
        "\n",
        "file = open(name, \"rb\")\n",
        "data = np.fromfile(file, '<f4')  \n",
        "print('Data shape : '+ str(data.shape))\n",
        "arr_size = data.shape[0]\n",
        "data1 = []\n",
        "data1.append(data)\n",
        "\n",
        "down = [0,1]\n",
        "\n",
        "# file_pat =  '*steps_50offset.npy'\n",
        "dwn_labels = np.array( ['101100', '101200', '121100', '131100', '131200', '131210',\n",
        "                        '131700', '151100', '201300', '201400', '202100', '205100',\n",
        "                        '205200', '231000', '231900', '233100', '302000'])\n",
        "not_dwn_labels = np.array(['101101', '101201', '121101',\n",
        "                           '131701', '132101', '132201', \n",
        "                           '133101', '151101', '202101'] )\n",
        "#[A B C AB BC CA ABC AB BC CA ABC]\n",
        "dwn_phase_labels     = np.array([0, 1, 2, 3, 4, 5, 6, 3, 4, 5, 6])\n",
        "#[A B C AB BC CA ABC ABN]\n",
        "not_dwn_phase_labels = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "#[AN BN CN AB BC CA ABC ABN BCN CAN ABCN]\n",
        "dwn_ground_labels     = np.array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])\n",
        "# Ground is always unk for non downstream\n",
        "not_dwn_ground_labels = np.array([3, 3, 3, 3, 3, 3, 3])\n",
        "\n",
        "class Indx(IntEnum):\n",
        "    ID = 0\n",
        "    MONITOR = 1\n",
        "    WAVEFORM = 2\n",
        "    FILE = 3\n",
        "    CLASS = 4\n",
        "    POSITION = 5\n",
        "    PHASE = 6\n",
        "    GROUND = 7\n",
        "    FREQ = 8\n",
        "    SECONDS = 9\n",
        "    FLAG = 10\n",
        "    DELTA_PT = 11\n",
        "    THREE_WIRE = 12\n",
        "    BLOB = 13\n",
        "    RANDOM = 14\n",
        "    LAST_CHANNEL = 31\n",
        "class SIndx(IntEnum):\n",
        "    ID = 0\n",
        "    NAME = 1\n",
        "    PT_S_A = 2\n",
        "    PT_S_B = 3\n",
        "    PT_S_C = 4\n",
        "    PT_P_A = 5\n",
        "    PT_P_B = 6\n",
        "    PT_P_C = 7\n",
        "    CT_S_A = 8\n",
        "    CT_S_B = 9\n",
        "    CT_S_C = 10\n",
        "    CT_P_A = 11\n",
        "    CT_P_B = 12\n",
        "    CT_P_C = 13\n",
        "    VA_SCALE = 14\n",
        "    VB_SCALE = 15\n",
        "    VC_SCALE = 16\n",
        "    IA_SCALE = 17\n",
        "    IB_SCALE = 18\n",
        "    IC_SCALE = 19\n",
        "    PQ_SCALE = 99\n",
        "    NO_SCALE = 100\n",
        "class Channels(IntEnum):\n",
        "    VA    =    0\n",
        "    VB    =    1\n",
        "    VC    =    2\n",
        "    IA    =    3\n",
        "    IB    =    4\n",
        "    IC    =    5\n",
        "    IN    =    6\n",
        "    VA_PD =    7\n",
        "    VB_PD =    8\n",
        "    VC_PD =    9\n",
        "    IA_PD =    10\n",
        "    IB_PD =    11\n",
        "    IC_PD =    12\n",
        "    IN_PD =    13\n",
        "    PA =       14\n",
        "    PB =       15\n",
        "    PC =       16\n",
        "    QA =       17\n",
        "    QB =       18\n",
        "    QC =       19\n",
        "    VA_SD =    20\n",
        "    VB_SD =    21\n",
        "    VC_SD =    22\n",
        "    IA_SD =    23\n",
        "    IB_SD =    24\n",
        "    IC_SD =    25\n",
        "    IN_SD =    26\n",
        "    VA_EVENS = 27\n",
        "    VB_EVENS = 28\n",
        "    VC_EVENS = 29\n",
        "    IA_EVENS = 30\n",
        "    IB_EVENS = 31\n",
        "    IC_EVENS = 32\n",
        "    IN_EVENS = 33\n",
        "    VA_ODDS =  34\n",
        "    VB_ODDS =  35\n",
        "    VC_ODDS =  36\n",
        "    IA_ODDS =  37\n",
        "    IB_ODDS =  38\n",
        "    IC_ODDS =  39\n",
        "    IN_ODDS =  40\n",
        "    \n",
        "    VA_NONS = 41\n",
        "    VB_NONS = 42\n",
        "    VC_NONS = 43\n",
        "    IA_NONS = 44\n",
        "    IB_NONS = 45\n",
        "    IC_NONS = 46\n",
        "    IN_NONS = 47\n",
        "    VAB = 48\n",
        "    VBC = 49\n",
        "    VCA = 50\n",
        "    \n",
        "class ce(IntEnum):\n",
        "    v = 0\n",
        "    i = 3\n",
        "    v_pd = 6\n",
        "    i_pd = 9\n",
        "    v_sd = 12\n",
        "    i_sd = 15\n",
        "    v_evens = 18\n",
        "    i_evens = 21\n",
        "    v_odds = 24\n",
        "    i_odds = 27\n",
        "    p = 30\n",
        "    q = 33\n",
        "\n",
        "class used_chnls(IntEnum):\n",
        "    VA    =    0\n",
        "    VB    =    1\n",
        "    VC    =    2\n",
        "    IA    =    3\n",
        "    IB    =    4\n",
        "    IC    =    5\n",
        "    IN    =    6\n",
        "    VA_PD =    7\n",
        "    VB_PD =    8\n",
        "    VC_PD =    9\n",
        "    IA_PD =    10\n",
        "    IB_PD =    11\n",
        "    IC_PD =    12\n",
        "    IN_PD =    13\n",
        "    IA_EVENS = 14\n",
        "    IB_EVENS = 15\n",
        "    IC_EVENS = 16\n",
        "    IN_EVENS = 17\n",
        "    PA =       18\n",
        "    PB =       19\n",
        "    PC =       20\n",
        "    QA =       21\n",
        "    QB =       22\n",
        "    QC =       23\n",
        "    IA_NONS = 24\n",
        "    IB_NONS = 25\n",
        "    IC_NONS = 26\n",
        "    IN_NONS = 27\n",
        "    VAB = 28\n",
        "    VBC = 29\n",
        "    VCA = 30\n",
        "\n",
        "def get_raw_vector_bulk(records_in, channel):\n",
        "    arr = np.frombuffer(records_in[-1], dtype=np.float32)\n",
        "    leng = int(arr_size/51)#arr[Indx.SECONDS] * arr[Indx.FREQ]\n",
        "    # array size constant 660 #######change\n",
        "    vect_size  = arr[(leng*channel):(leng*(channel + 1))]\n",
        "    vect = vect_size#[start:end]\n",
        "    return vect\n",
        "\n",
        "def all_channel_input(input_data = data1):\n",
        "    new_result = dict()\n",
        "    channels = ['VA','VB','VC',\n",
        "                'IA','IB','IC','IN',\n",
        "                'VA_PD','VB_PD','VC_PD',\n",
        "                'IA_PD','IB_PD','IC_PD','IN_PD',\n",
        "                'IA_EVENS','IB_EVENS','IC_EVENS','IN_EVENS',\n",
        "                'PA','PB','PC',\n",
        "                'QA','QB','QC',\n",
        "                'IA_NONS','IB_NONS','IC_NONS','IN_NONS',\n",
        "                'VAB','VBC','VCA'           \n",
        "    ]\n",
        "    \n",
        "    rms_raw_vecs = []\n",
        "\n",
        "    for j in channels:\n",
        "        \n",
        "        if j == 'VA':\n",
        "            channel=Channels.VA\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j == 'VB':\n",
        "            channel=Channels.VB\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VC':\n",
        "            channel=Channels.VC\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "        #---\n",
        "        elif j == 'IA':\n",
        "            channel=Channels.IA\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB':\n",
        "            channel=Channels.IB\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC':\n",
        "            channel=Channels.IC\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN':\n",
        "            channel=Channels.IN\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        #---\n",
        "        elif j == 'VA_PD':\n",
        "            channel=Channels.VA_PD\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j == 'VB_PD':\n",
        "            channel=Channels.VB_PD\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VC_PD':\n",
        "            channel=Channels.VC_PD\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "        #---\n",
        "        elif j == 'IA_PD':\n",
        "            channel=Channels.IA_PD\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_PD':\n",
        "            channel=Channels.IB_PD\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_PD':\n",
        "            channel=Channels.IC_PD\n",
        "            scale_indx = SIndx.IC_SCALE    \n",
        "        elif j =='IN_PD':\n",
        "            channel=Channels.IN_PD\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        \n",
        "        #---\n",
        "        elif j == 'IA_EVENS':\n",
        "            channel=Channels.IA_EVENS\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_EVENS':\n",
        "            channel=Channels.IB_EVENS\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_EVENS':\n",
        "            channel=Channels.IC_EVENS\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN_EVENS':\n",
        "            channel=Channels.IN_EVENS\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "            \n",
        "        #---\n",
        "        elif j =='PA':\n",
        "            channel=Channels.PA\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j =='PB':\n",
        "            channel=Channels.PB\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j =='PC':\n",
        "            channel=Channels.PC\n",
        "            scale_indx = SIndx.VC_SCALE                \n",
        "        #---\n",
        "        elif j =='QA':\n",
        "            channel=Channels.QA\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j =='QB':\n",
        "            channel=Channels.QB\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j =='QC':\n",
        "            channel=Channels.QC\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "\n",
        "        #---    \n",
        "        elif j =='IA_NONS':\n",
        "            channel=Channels.IA_NONS\n",
        "            scale_indx = SIndx.IA_SCALE\n",
        "        elif j == 'IB_NONS':\n",
        "            channel=Channels.IB_NONS\n",
        "            scale_indx = SIndx.IB_SCALE\n",
        "        elif j == 'IC_NONS':\n",
        "            channel=Channels.IC_NONS\n",
        "            scale_indx = SIndx.IC_SCALE\n",
        "        elif j == 'IN_NONS':\n",
        "            channel=Channels.IN_NONS\n",
        "            scale_indx = SIndx.NO_SCALE\n",
        "        #---\n",
        "        elif j =='VAB':\n",
        "            channel=Channels.VAB\n",
        "            scale_indx = SIndx.VA_SCALE\n",
        "        elif j =='VBC':\n",
        "            channel=Channels.VBC\n",
        "            scale_indx = SIndx.VB_SCALE\n",
        "        elif j == 'VCA':\n",
        "            channel=Channels.VCA\n",
        "            scale_indx = SIndx.VC_SCALE\n",
        "            \n",
        "            \n",
        "                \n",
        "        vect = get_raw_vector_bulk(records_in = data1, channel=channel)\n",
        "        rms_raw_vecs.append(vect)\n",
        "            \n",
        "    return rms_raw_vecs\n",
        "i_params ={\n",
        "    'small': 10,\n",
        "    'med_start': 100,\n",
        "    'med_end': 500,\n",
        "    'big': 1000\n",
        "    }\n",
        "\n",
        "pq_params ={\n",
        "    'small': 10000,\n",
        "    'med_start': 50000,\n",
        "    'med_end': 100000,\n",
        "    'big': 500000\n",
        "    }\n",
        "\n",
        "def fuzzy_scale(val, params):\n",
        "    ret = 0.0\n",
        "    if val <= params['small']:\n",
        "        ret = 0.33\n",
        "    elif val < params['med_start']:\n",
        "        ret  = 0.33 + (val - params['small']) * (0.33/(params['med_start'] - params['small']))\n",
        "    elif val <= params['med_end']:\n",
        "        ret = 0.66\n",
        "    elif val < params['big']:\n",
        "        ret  = 0.66 + (val - params['med_end']) * (0.33/(params['big'] - params['med_end']))\n",
        "    else:\n",
        "        ret = 1.0\n",
        "    return ret\n",
        "\n",
        "############ ends ##############\n",
        "\n",
        "def normalize(rms_vec,new_max = 1):\n",
        "        #change as per requirement, default selected sliced index for test purpose\n",
        "    _all = rms_vec\n",
        "\n",
        "    \n",
        "### new normalization ###\n",
        "    ########################################## VA,VB,VC ##########################\n",
        "    vabc_all = np.stack((_all[used_chnls.VA],_all[used_chnls.VB],_all[used_chnls.VC]))\n",
        "    \n",
        "    vabc_max = np.max(vabc_all)\n",
        "    vabc_min = np.min(vabc_all)\n",
        "    v_range = vabc_max - vabc_min\n",
        "    if new_max == 0:\n",
        "        if v_range < 1e-10 or vabc_max < 1:\n",
        "            print('found vabc_max ' + str(v_range)+' <0 or '+ str(vabc_max)+' <0' )\n",
        "            norm_va,norm_vb,norm_vc = ['x'],['x'],['x']\n",
        "            v_bias = ['x']\n",
        "        else:\n",
        "            #skip the record\n",
        "            # vabc_max if <1 skip this\n",
        "            v_scale = v_range/vabc_max\n",
        "            norm_va = (_all[used_chnls.VA] - vabc_min)/(v_range)\n",
        "            norm_vb = (_all[used_chnls.VB] - vabc_min)/(v_range)\n",
        "            norm_vc = (_all[used_chnls.VC] - vabc_min)/(v_range)\n",
        "            v_bias = [v_scale] * 120\n",
        "    else:\n",
        "        if v_range < 1e-10 or vabc_max < 1:\n",
        "            print('found ' + str(v_range)+' <0 or '+ str(vabc_max)+' <0' )\n",
        "            norm_va,norm_vb,norm_vc = ['x'],['x'],['x']\n",
        "            v_bias = ['x']\n",
        "        else:\n",
        "            v_scale = v_range/vabc_max\n",
        "            norm_va = (_all[used_chnls.VA])/(vabc_max)\n",
        "            norm_vb = (_all[used_chnls.VB])/(vabc_max)\n",
        "            norm_vc = (_all[used_chnls.VC])/(vabc_max)\n",
        "            v_bias = [v_scale] * 120\n",
        "    \n",
        "    ########################################## IA,IB,IC ##########################\n",
        "    iabc_all = np.stack((_all[used_chnls.IA],_all[used_chnls.IB],_all[used_chnls.IC],_all[used_chnls.IN]))\n",
        "    \n",
        "    iabc_max = np.max(iabc_all)\n",
        "    iabc_min = np.min(iabc_all)\n",
        "    i_range = iabc_max - iabc_min #skip the record if < 0.001\n",
        "    if new_max == 0:\n",
        "        if i_range < 1e-20 or iabc_max < 1e-10:\n",
        "            print('found i_range' + str(i_range)+' <0.001'  )\n",
        "            norm_ia,norm_ib,norm_ic,norm_in = ['x'],['x'],['x'],['x']\n",
        "            i_bias = ['x']\n",
        "        else:\n",
        "            i_scale = fuzzy_scale(i_range, i_params)\n",
        "            norm_ia = (_all[used_chnls.IA] - iabc_min)/(i_range)\n",
        "            norm_ib = (_all[used_chnls.IB] - iabc_min)/(i_range)\n",
        "            norm_ic = (_all[used_chnls.IC] - iabc_min)/(i_range)\n",
        "            norm_in = (_all[used_chnls.IN] - iabc_min)/(i_range)\n",
        "            i_bias = [i_scale] * 120\n",
        "    else:\n",
        "        if i_range < 1e-20 or iabc_max < 1e-10:\n",
        "            print('found ' + str(i_range)+' <0.001'  )\n",
        "            norm_ia,norm_ib,norm_ic,norm_in = ['x'],['x'],['x'],['x']\n",
        "            i_bias = ['x']\n",
        "        else:\n",
        "            i_scale = fuzzy_scale(i_range, i_params)\n",
        "            norm_ia = (_all[used_chnls.IA])/(iabc_max)\n",
        "            norm_ib = (_all[used_chnls.IB])/(iabc_max)\n",
        "            norm_ic = (_all[used_chnls.IC])/(iabc_max)\n",
        "            norm_in = (_all[used_chnls.IN])/(iabc_max)\n",
        "            i_bias = [i_scale]* 120\n",
        "            \n",
        "    \n",
        "    \n",
        "    #################################### 'VA_PD','VB_PD','VC_PD'  #############################\n",
        "    PD_vabc_all =  np.stack((_all[used_chnls.VA_PD],_all[used_chnls.VB_PD],_all[used_chnls.VC_PD]))\n",
        "    \n",
        "    PD_vabc_max = np.max(PD_vabc_all)\n",
        "    PD_vabc_min = np.min(PD_vabc_all)\n",
        "    v_pd_scale = PD_vabc_max/vabc_max #vabc_max<1 skip and PD_vabc_max< 1\n",
        "    if PD_vabc_max < 1e-10:\n",
        "        print('found PD_vabc_max',str(vabc_max),str(PD_vabc_max), ' < 1'  )\n",
        "        norm_va_pd,norm_vb_pd,norm_vc_pd = ['x'],['x'],['x']\n",
        "        vpd_bias = ['x'] \n",
        "    else: \n",
        "        norm_va_pd = (_all[used_chnls.VA_PD] )/( PD_vabc_max)\n",
        "        norm_vb_pd = (_all[used_chnls.VB_PD] )/( PD_vabc_max)\n",
        "        norm_vc_pd = (_all[used_chnls.VC_PD] )/( PD_vabc_max)\n",
        "        vpd_bias = [v_pd_scale]* 120\n",
        "    \n",
        "    ################################  IA_PD,IB_PD,IC_PD,IN_PD  #############################\n",
        "    PD_iabc_all = np.stack((_all[used_chnls.IA_PD],_all[used_chnls.IB_PD],_all[used_chnls.IC_PD],_all[used_chnls.IN_PD]))\n",
        "    \n",
        "    PD_iabc_max = np.max(PD_iabc_all)\n",
        "    PD_iabc_min = np.min(PD_iabc_all)\n",
        "    if PD_iabc_max<1e-10:\n",
        "        print('found PD_iabc_max ' + str(PD_iabc_max)+' <0.001'  )\n",
        "        norm_ia_pd,norm_ib_pd,norm_ic_pd,norm_ic_pd = ['x'],['x'],['x'],['x']\n",
        "        ipd_bias = ['x'] \n",
        "    else:\n",
        "        i_pd_scale = fuzzy_scale(PD_iabc_max, i_params)\n",
        "        norm_ia_pd = (_all[used_chnls.IA_PD] )/(PD_iabc_max)# PD_iabc_max <0.001\n",
        "        norm_ib_pd = (_all[used_chnls.IB_PD])/(PD_iabc_max)\n",
        "        norm_ic_pd = (_all[used_chnls.IC_PD])/(PD_iabc_max)\n",
        "        norm_in_pd = (_all[used_chnls.IN_PD])/(PD_iabc_max)\n",
        "        ipd_bias = [i_pd_scale]* 120\n",
        "    \n",
        "    ################################# I_EVEN & I_ODD  ####################################\n",
        "    ## I_EVEN\n",
        "    even_iabc_all = np.stack((_all[used_chnls.IA_EVENS],_all[used_chnls.IB_EVENS],_all[used_chnls.IC_EVENS],_all[used_chnls.IN_EVENS]))\n",
        "    \n",
        "    even_iabc_max = np.max(even_iabc_all)\n",
        "    even_iabc_min = np.min(even_iabc_all)\n",
        "    if even_iabc_max<1e-10:\n",
        "        print('found even_iabc_max ' + str(even_iabc_all)+' <0.e-10'  )\n",
        "        norm_ia_even,norm_ib_even,norm_ic_even,norm_in_even = ['x'],['x'],['x'],['x']\n",
        "        ieven_bias = ['x'] \n",
        "    else:\n",
        "        i_even_scale = fuzzy_scale(even_iabc_max, i_params)\n",
        "        norm_ia_even = (_all[used_chnls.IA_EVENS] )/(even_iabc_max)# even_iabc_max <0.001\n",
        "        norm_ib_even = (_all[used_chnls.IB_EVENS])/(even_iabc_max)\n",
        "        norm_ic_even = (_all[used_chnls.IC_EVENS])/(even_iabc_max)\n",
        "        norm_in_even = (_all[used_chnls.IN_EVENS])/(even_iabc_max)\n",
        "        ieven_bias = [i_even_scale] * 120\n",
        "        \n",
        "    #######################################   PA,PB,PC  ##################################\n",
        "    pabc_all = np.stack((_all[used_chnls.PA],_all[used_chnls.PB],_all[used_chnls.PC]))\n",
        "    \n",
        "    pabc_min = np.min(pabc_all)\n",
        "    pabc_max = np.max(pabc_all)\n",
        "    p_range = pabc_max - pabc_min # p_range<1 skip\n",
        "    if p_range<0.001:\n",
        "        print('found p_range' + str(p_range)+' <1'  )\n",
        "        norm_pa,norm_pb,norm_pc = ['x'],['x'],['x']\n",
        "        p_bias = ['x']\n",
        "    else:\n",
        "        p_scale = fuzzy_scale(p_range, pq_params)\n",
        "        norm_pa = (_all[used_chnls.PA] - float(pabc_min))/p_range\n",
        "        norm_pb = (_all[used_chnls.PB] - float(pabc_min))/p_range\n",
        "        norm_pc = (_all[used_chnls.PC] - float(pabc_min))/p_range\n",
        "        p_bias = [p_scale] * 120\n",
        "\n",
        "    # QA,QB,QC\n",
        "    qabc_all = np.stack((_all[used_chnls.QA],_all[used_chnls.QB],_all[used_chnls.QC]))\n",
        "    qabc_min = np.min(qabc_all)\n",
        "    qabc_max = np.max(qabc_all)\n",
        "    q_range = qabc_max - qabc_min # q_range<1 skip \n",
        "    if q_range< 0.001:\n",
        "        print('found q_range' + str(q_range)+' <1'  )\n",
        "        norm_qa,norm_qb,norm_qc = ['x'],['x'],['x']\n",
        "        q_bias = ['x'] \n",
        "    else:  \n",
        "        q_scale = fuzzy_scale(q_range, pq_params)\n",
        "        norm_qa= (_all[used_chnls.QA] - qabc_min)/q_range\n",
        "        norm_qb= (_all[used_chnls.QB] - qabc_min)/q_range\n",
        "        norm_qc= (_all[used_chnls.QC] - qabc_min)/q_range\n",
        "        q_bias = [q_scale] * 120\n",
        "    \n",
        "    ########################## 'IA_NONS','IB_NONS','IC_NONS','IN_NONS' #########################\n",
        "    nons_iabc_all = np.stack((_all[used_chnls.IA_NONS],_all[used_chnls.IB_NONS],_all[used_chnls.IC_NONS],_all[used_chnls.IN_NONS]))\n",
        "\n",
        "    nons_iabc_max = np.max(nons_iabc_all)\n",
        "    nons_iabc_min = np.min(nons_iabc_all)\n",
        "    if nons_iabc_max<0.001:\n",
        "        print('found ' + str(nons_iabc_max)+' <0.001'  )\n",
        "        norm_ia_nons,norm_ib_nons,norm_ic_nons,norm_in_nons = ['x'],['x'],['x'],['x']\n",
        "        inons_bias = ['x'] \n",
        "    else:\n",
        "        i_nons_scale = fuzzy_scale(nons_iabc_max, i_params)\n",
        "        norm_ia_nons = (_all[used_chnls.IA_NONS] )/(nons_iabc_max)\n",
        "        norm_ib_nons = (_all[used_chnls.IB_NONS])/(nons_iabc_max)\n",
        "        norm_ic_nons = (_all[used_chnls.IC_NONS])/(nons_iabc_max)\n",
        "        norm_in_nons = (_all[used_chnls.IN_NONS])/(nons_iabc_max)\n",
        "        inons_bias = [i_nons_scale] * 120\n",
        "        \n",
        "    ##################################### 'VAB','VBC','VCA'  ###################################\n",
        "    vabc_ll_all =  np.stack((_all[used_chnls.VAB],_all[used_chnls.VBC],_all[used_chnls.VCA]))\n",
        "    vabc_ll_max = np.max(vabc_ll_all)\n",
        "    vabc_ll_min = np.min(vabc_ll_all)\n",
        "    v_ll_scale = vabc_ll_max/vabc_max #skip vabc_ll_max < 1\n",
        "    if vabc_ll_max<1:\n",
        "        print('found vabc_ll_max '+str(vabc_ll_max)+' <1'  )\n",
        "        norm_va_ll,norm_vb_ll,norm_vc_ll = ['x'],['x'],['x']\n",
        "        vll_bias = ['x'] \n",
        "    else: \n",
        "        norm_va_ll = (_all[used_chnls.VAB] )/( vabc_ll_max)\n",
        "        norm_vb_ll = (_all[used_chnls.VBC] )/( vabc_ll_max)\n",
        "        norm_vc_ll = (_all[used_chnls.VCA] )/( vabc_ll_max)\n",
        "        vll_bias = [v_ll_scale] * 120\n",
        "\n",
        "    ################################## ARRAY STACK ####################################\n",
        "    \n",
        "    bias_list = np.array([np.array(v_bias),np.array(i_bias),\n",
        "                     np.array(vpd_bias) ,np.array(ipd_bias) ,np.array(ieven_bias) ,\n",
        "                     np.array(p_bias),np.array(q_bias) ,\n",
        "                     np.array(inons_bias),np.array(vll_bias)])\n",
        "        \n",
        "        \n",
        "    final_norm = np.array([ norm_va,norm_vb,norm_vc,# np.array(v_bias)\n",
        "                       norm_ia,norm_ib,norm_ic, norm_in,#np.array(i_bias)\n",
        "                       norm_va_pd,norm_vb_pd,norm_vc_pd,# np.array(vpd_bias)\n",
        "                       norm_ia_pd,norm_ib_pd,norm_ic_pd,norm_in_pd, #np.array(ipd_bias)\n",
        "                       norm_ia_even,norm_ib_even,norm_ic_even,norm_in_even,#np.array(ieven_bias)\n",
        "                       norm_pa,norm_pb,norm_pc,# np.array(p_bias)\n",
        "                       norm_qa,norm_qb,norm_qc,#  np.array(q_bias)\n",
        "                       norm_ia_nons,norm_ib_nons,norm_ic_nons,norm_in_nons,\n",
        "                       norm_va_ll,norm_vb_ll,norm_vc_ll\n",
        "                    ])\n",
        "    return final_norm, bias_list\n",
        "\n",
        "def extract_tensor(data,sesson):\n",
        "    sess = sesson\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    output_name = sess.get_outputs()[0].name\n",
        "    res = sess.run([output_name], {input_name: data.astype(np.float32)})\n",
        "    return res[0]\n",
        "\n",
        "def pyt_predict_phase(input_name,sesson):\n",
        "    sess = sesson\n",
        "    input1 = input_name\n",
        "    input_name1 = sess.get_inputs()[0].name\n",
        "    input_name2 = sess.get_inputs()[1].name\n",
        "    output_name = sess.get_outputs()[0].name\n",
        "    res = sess.run([output_name],{input_name1:input1[1].astype(np.float32),input_name2:input1[0].astype(np.float32)})\n",
        "    return res[0]\n",
        "\n",
        "def predict_label():\n",
        "    predictions = []\n",
        "    \n",
        "    user_id0 = user_ids[0]\n",
        "    root_model_path0= '/content/drive/MyDrive/capstone/Feb2023_deployment/version1/'\n",
        "    version0=prev_version_models\n",
        "    \n",
        "    user_id1 = user_ids[1]\n",
        "    root_model_path1= '/content/drive/MyDrive/capstone/Feb2023_deployment/version2/'\n",
        "    version1=current_version_models\n",
        "    \n",
        "    process1 = psutil.Process(os.getpid())\n",
        "    vect = get_raw_vector_bulk(data1,channel = Channels.VA) \n",
        "    logging.debug('process1 - {}' .format(process1.memory_info().rss)) \n",
        "\n",
        "    process2 = psutil.Process(os.getpid())\n",
        "    rms_raw_vecs = all_channel_input(data1)\n",
        "    logging.debug('process2 - {}' .format(process2.memory_info().rss))\n",
        "\n",
        "    process3 = psutil.Process(os.getpid())\n",
        "    scaled_x,bias_x = np.array(normalize(rms_vec=rms_raw_vecs))\n",
        "    logging.debug('process3 - {}' .format(process3.memory_info().rss)) \n",
        "    # Check for nans\n",
        "    is_nan = len(np.argwhere(np.isnan(bias_x)))\n",
        "    if is_nan == 0:\n",
        "        for xx in scaled_x :\n",
        "            is_nan = len(np.argwhere(np.isnan(xx))) \n",
        "            if is_nan > 0:\n",
        "                break\n",
        "\n",
        "    # Proceed on no nans\n",
        "    if is_nan == 0:\n",
        "      # Adding the multi step embedded features needed for LSTM\n",
        "        len_x = 120 * (len(scaled_x[0]) // 120)\n",
        "\n",
        "        #Compute number of possible time steps\n",
        "        starts = np.array(range(0,len_x - 120 + 1, 50))\n",
        "        ts = len(starts)\n",
        "\n",
        "\n",
        "      # Gather data in time steps\n",
        "        main_list = []\n",
        "        for start in starts:\n",
        "            end = int(start)+120\n",
        "            final_norm = np.vstack((bias_x[0], scaled_x[0][start:end],scaled_x[1][start:end],scaled_x[2][start:end],# np.array(v_bias_x)\n",
        "                          bias_x[1] , scaled_x[3][start:end],scaled_x[4][start:end],scaled_x[5][start:end], scaled_x[6][start:end],#np.array(i_bias_x)\n",
        "                            bias_x[2]  , scaled_x[7][start:end],scaled_x[8][start:end],scaled_x[9][start:end],# np.array(vpd_bias_x)\n",
        "                              bias_x[3] , scaled_x[10][start:end],scaled_x[11][start:end],scaled_x[12][start:end],scaled_x[13][start:end], #np.array(ipd_bias_x)\n",
        "                                  bias_x[4] , scaled_x[14][start:end],scaled_x[15][start:end],scaled_x[16][start:end],scaled_x[17][start:end],#np.array(ieven_bias_x)\n",
        "                                          bias_x[5] , scaled_x[18][start:end],scaled_x[19][start:end],scaled_x[20][start:end],# np.array(p_bias_x)\n",
        "                                            bias_x[6] ,  scaled_x[21][start:end],scaled_x[22][start:end],scaled_x[23][start:end],#  np.array(q_bias_x)\n",
        "                                                bias_x[7], scaled_x[24][start:end],scaled_x[25][start:end],scaled_x[26][start:end],scaled_x[27][start:end],\n",
        "                                                  bias_x[8], scaled_x[28][start:end],scaled_x[29][start:end],scaled_x[30][start:end])\n",
        "            ).T\n",
        "            main_list.append(final_norm)\n",
        "        sliced_x = np.array(main_list)\n",
        "        # Determine postion\n",
        "        process4 = psutil.Process(os.getpid())\n",
        "        \n",
        "        sess1_prev = ort.InferenceSession(root_model_path0 + version0[0]+\".onnx\")\n",
        "        sess1_new = ort.InferenceSession(root_model_path1 + version1[0]+\".onnx\")\n",
        "        \n",
        "        pos_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "        pos_x0 = pos_x0.reshape((1, pos_x0.shape[0], pos_x0.shape[1]))\n",
        "        pos_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "        pos_x1 = pos_x1.reshape((1, pos_x1.shape[0], pos_x1.shape[1]))\n",
        "        \n",
        "        sess2_prev = ort.InferenceSession(root_model_path0 + version0[1]+\".onnx\")\n",
        "        sess2_new = ort.InferenceSession(root_model_path1 + version1[1]+\".onnx\")\n",
        "        pos_y0 = extract_tensor(pos_x0,sess2_prev)\n",
        "        pos_y0 = pos_y0.tolist()\n",
        "        pos_y0 = pos_y0[0][0]\n",
        "        pos_y0 = 0 if pos_y0 < 0.5 else 1 \n",
        "        \n",
        "        pos_y1 = extract_tensor(pos_x1,sess2_new)\n",
        "        pos_y1 = pos_y1.tolist()\n",
        "        pos_y1 = pos_y1[0][0]\n",
        "        pos_y1 = 0 if pos_y1 < 0.5 else 1 \n",
        "        logging.debug('process4 - {}' .format(process4.memory_info().rss)) \n",
        "\n",
        "        process5 = psutil.Process(os.getpid())\n",
        "        sess1_prev = ort.InferenceSession(root_model_path0 + version0[2]+\".onnx\")\n",
        "        sess1_new = ort.InferenceSession(root_model_path1 + version1[2]+\".onnx\")\n",
        "        \n",
        "        twacs_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "        twacs_x0 = twacs_x0.reshape((1, twacs_x0.shape[0], twacs_x0.shape[1])) \n",
        "        twacs_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "        twacs_x1 = twacs_x1.reshape((1, twacs_x1.shape[0], twacs_x1.shape[1])) \n",
        "        \n",
        "        sess2_prev = ort.InferenceSession(root_model_path0 + version0[3]+\".onnx\")\n",
        "        sess2_new = ort.InferenceSession(root_model_path1 + version1[3]+\".onnx\")\n",
        "        twacs_y0 = extract_tensor(twacs_x0, sess2_prev)\n",
        "        twacs_y0 = 0 if pos_y0 < 0.7 else 1\n",
        "        twacs_y1 = extract_tensor(twacs_x1, sess2_new)\n",
        "        twacs_y1 = 0 if pos_y1 < 0.7 else 1\n",
        "        \n",
        "        logging.debug('process5 - {}' .format(process5.memory_info().rss)) \n",
        "\n",
        "\n",
        "        if pos_y0 == 0:\n",
        "            process6 = psutil.Process(os.getpid())\n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[4]+\".onnx\")\n",
        "            \n",
        "            dwn_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "            dwn_x0 = dwn_x0.reshape((1, dwn_x0.shape[0], dwn_x0.shape[1]))\n",
        "            \n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[5]+\".onnx\") \n",
        "            class_y0= extract_tensor(dwn_x0, sess2_prev)\n",
        "            logging.debug('process6 - {}' .format(process6.memory_info().rss)) \n",
        "                \n",
        "            # intermediate operations to predict phase\n",
        "            process7 = psutil.Process(os.getpid())\n",
        "            indy0 = np.argmax(class_y0)\n",
        "            class_y0 = np.zeros(class_y0.shape)\n",
        "            class_y0[0, indy0] = 1\n",
        "            \n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[6]+\".onnx\")\n",
        "            phase_dwn_x0 = pyt_predict_phase([sliced_x,class_y0],sesson=sess1_prev)\n",
        "            phase_dwn_x0 = np.hstack((np.tile(class_y0, (phase_dwn_x0.shape[0], 1)), phase_dwn_x0))\n",
        "            phase_dwn_x0 = phase_dwn_x0.reshape((1,phase_dwn_x0.shape[0],phase_dwn_x0.shape[1]))\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[7]+\".onnx\")\n",
        "            phase_y0 = extract_tensor(phase_dwn_x0,sess2_prev)\n",
        "            logging.debug('process7 - {}' .format(process7.memory_info().rss))  \n",
        "            \n",
        "            pred_label0 = dwn_labels[class_y0.argmax(axis=1)]\n",
        "            pred_phase0  = dwn_phase_labels[phase_y0.argmax(axis=1)]\n",
        "            pred_ground0 = dwn_ground_labels[phase_y0.argmax(axis=1)]\n",
        "           \n",
        "            predictions.append(pred_label0)\n",
        "            predictions.append(pred_phase0)\n",
        "            predictions.append(pred_ground0)\n",
        "            predictions.append(twacs_y0)\n",
        "            predictions.append(pos_y0)\n",
        "\n",
        "        elif pos_y0==1:\n",
        "            process8 = psutil.Process(os.getpid())\n",
        "            sess1_prev = ort.InferenceSession(root_model_path0 + version0[8]+\".onnx\")\n",
        "            \n",
        "            not_dwn_x0 = extract_tensor(sliced_x,sess1_prev)\n",
        "            not_dwn_x0 = not_dwn_x0.reshape((1, not_dwn_x0.shape[0], not_dwn_x0.shape[1]))  \n",
        "            \n",
        "            \n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[9]+\".onnx\")\n",
        "            \n",
        "            class_y0= extract_tensor(not_dwn_x0, sess2_prev)\n",
        "            logging.debug('process8 - {}' .format(process8.memory_info().rss))\n",
        "\n",
        "            # intermediate operations to predict phase\n",
        "            process9 = psutil.Process(os.getpid())\n",
        "            sliced_x[:,:, mask_1] = 0.0\n",
        "            sliced_x[:,:, mask_2] = 0.0 \n",
        "            \n",
        "            indy0 = np.argmax(class_y0)\n",
        "            class_y0 = np.zeros(class_y0.shape)\n",
        "            class_y0[0, indy0] = 1\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[10]+\".onnx\")\n",
        "            phase_not_dwn_x0 = pyt_predict_phase(input_name=[sliced_x,class_y0],sesson=sess2_prev)\n",
        "            phase_not_dwn_x0 = np.hstack((np.tile(class_y0, (phase_not_dwn_x0.shape[0], 1)), phase_not_dwn_x0))\n",
        "            phase_not_dwn_x0 = phase_not_dwn_x0.reshape((1,phase_not_dwn_x0.shape[0], phase_not_dwn_x0.shape[1]))\n",
        "            sess2_prev = ort.InferenceSession(root_model_path0 + version0[11]+\".onnx\")\n",
        "            phase_y0 = extract_tensor(phase_not_dwn_x0,sess2_prev)\n",
        "            logging.debug('process9 - {}' .format(process9.memory_info().rss))  \n",
        "\n",
        "            pred_label0 = not_dwn_labels[class_y0.argmax(axis=1)]\n",
        "            pred_phase0  = not_dwn_phase_labels[phase_y0.argmax(axis=1)]\n",
        "            pred_ground0 = not_dwn_ground_labels[phase_y0.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label0)\n",
        "            predictions.append(pred_phase0)\n",
        "            predictions.append(pred_ground0)\n",
        "            predictions.append(twacs_y0)\n",
        "            predictions.append(pos_y0)\n",
        " \n",
        "        if pos_y1 == 0:\n",
        "            process6 = psutil.Process(os.getpid())\n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[4]+\".onnx\")\n",
        "            dwn_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "            dwn_x1 = dwn_x1.reshape((1, dwn_x1.shape[0], dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[5]+\".onnx\") \n",
        "            class_y1= extract_tensor(dwn_x1, sess2_new)\n",
        "            logging.debug('process6 - {}' .format(process6.memory_info().rss)) \n",
        "                \n",
        "            # intermediate operations to predict phase\n",
        "            process7 = psutil.Process(os.getpid())\n",
        "            indy1 = np.argmax(class_y1)\n",
        "            class_y1 = np.zeros(class_y1.shape)\n",
        "            class_y1[0, indy1] = 1\n",
        "            \n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[6]+\".onnx\")\n",
        "            phase_dwn_x1 = pyt_predict_phase([sliced_x,class_y1],sesson=sess1_new)\n",
        "            phase_dwn_x1 = np.hstack((np.tile(class_y1, (phase_dwn_x1.shape[0], 1)), phase_dwn_x1))\n",
        "            phase_dwn_x1 = phase_dwn_x1.reshape((1,phase_dwn_x1.shape[0],phase_dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[7]+\".onnx\")\n",
        "            phase_y1 = extract_tensor(phase_dwn_x1,sess2_new)\n",
        "            logging.debug('process7 - {}' .format(process7.memory_info().rss))  \n",
        "            pred_label1 = dwn_labels[class_y1.argmax(axis=1)]\n",
        "            pred_phase1 = dwn_phase_labels[phase_y1.argmax(axis=1)]\n",
        "            pred_ground1 = dwn_ground_labels[phase_y1.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label1)\n",
        "            predictions.append(pred_phase1)\n",
        "            predictions.append(pred_ground1)\n",
        "            predictions.append(twacs_y1)\n",
        "            predictions.append(pos_y1)\n",
        "\n",
        "        elif pos_y1==1:\n",
        "            process8 = psutil.Process(os.getpid())\n",
        "            sess1_new = ort.InferenceSession(root_model_path1 + version1[8]+\".onnx\")\n",
        "            not_dwn_x1 = extract_tensor(sliced_x,sess1_new)\n",
        "            not_dwn_x1 = not_dwn_x1.reshape((1, not_dwn_x1.shape[0], not_dwn_x1.shape[1]))  \n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[9]+\".onnx\")\n",
        "            \n",
        "            class_y1= extract_tensor(not_dwn_x1, sess2_new)\n",
        "            logging.debug('process8 - {}' .format(process8.memory_info().rss))\n",
        "\n",
        "            # intermediate operations to predict phase\n",
        "            process9 = psutil.Process(os.getpid())\n",
        "            sliced_x[:,:, mask_1] = 0.0\n",
        "            sliced_x[:,:, mask_2] = 0.0 \n",
        "            indy1 = np.argmax(class_y1)\n",
        "            class_y1 = np.zeros(class_y1.shape)\n",
        "            class_y1[0, indy1] = 1\n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[10]+\".onnx\")\n",
        "            phase_not_dwn_x1 = pyt_predict_phase(input_name=[sliced_x,class_y1],sesson=sess2_new)\n",
        "            phase_not_dwn_x1 = np.hstack((np.tile(class_y1, (phase_not_dwn_x1.shape[0], 1)), phase_not_dwn_x1))\n",
        "            phase_not_dwn_x1 = phase_not_dwn_x1.reshape((1,phase_not_dwn_x1.shape[0], phase_not_dwn_x1.shape[1]))\n",
        "            \n",
        "            sess2_new = ort.InferenceSession(root_model_path1 + version1[11]+\".onnx\")\n",
        "            phase_y1 = extract_tensor(phase_not_dwn_x1,sess2_new)\n",
        "            logging.debug('process9 - {}' .format(process9.memory_info().rss))\n",
        "            pred_label1 = not_dwn_labels[class_y1.argmax(axis=1)]\n",
        "            pred_phase1  = not_dwn_phase_labels[phase_y1.argmax(axis=1)]\n",
        "            pred_ground1 = not_dwn_ground_labels[phase_y1.argmax(axis=1)]\n",
        "            \n",
        "            predictions.append(pred_label1)\n",
        "            predictions.append(pred_phase1)\n",
        "            predictions.append(pred_ground1)\n",
        "            predictions.append(twacs_y1)\n",
        "            predictions.append(pos_y1)\n",
        "    return predictions\n",
        "    # return pred_label,pred_phase,pred_ground,twacs_y,pos_y\n",
        "if __name__=='__main__':\n",
        "    final_process = psutil.Process(os.getpid())\n",
        "    predictions = predict_label()\n",
        "    pred_label0 = predictions[0]\n",
        "    pred_phase0 = predictions[1]\n",
        "    pred_ground0 = predictions[2]\n",
        "    twacs_y0 = predictions[3]\n",
        "    pos_y0 =predictions[4]\n",
        "#### new preds\n",
        "    pred_label1 = predictions[5]\n",
        "    pred_phase1 =predictions[6]\n",
        "    pred_ground1 = predictions[7]\n",
        "    twacs_y1 = predictions[8]\n",
        "    pos_y1 = predictions[9]  \n",
        "    \n",
        "    pred_label0  = pred_label0[0][:-1]\n",
        "    print('pred_label prev ',pred_label0)\n",
        "    logging.debug('pred_label - {}' .format(pred_label0))\n",
        "    pred_phase0  = pred_phase0[0]\n",
        "    print('pred_phase prev',pred_phase0)\n",
        "    logging.debug('pred_phase - {}' .format(pred_phase0))\n",
        "    pred_ground0 = pred_ground0[0]\n",
        "    print('pred_ground prev',pred_ground0)\n",
        "    logging.debug('pred_ground - {}' .format(pred_ground0))\n",
        "    pred_twac0   = twacs_y0\n",
        "    print('pred_twac prev',pred_twac0)\n",
        "    logging.debug('pred_twac - {}' .format(pred_twac0))\n",
        "    pred_pos0    = pos_y0\n",
        "    print('pred_pos prev',pred_pos0)\n",
        "    logging.debug('pred_pos - {}' .format(pred_pos0))\n",
        "    print('***************** New Model Predictions **********************')\n",
        "    pred_label1  = pred_label1[0][:-1]\n",
        "    print('pred_label new',pred_label1)\n",
        "    logging.debug('pred_label - {}' .format(pred_label1))\n",
        "    pred_phase1  = pred_phase1[0]\n",
        "    print('pred_phase new ',pred_phase1)\n",
        "    logging.debug('pred_phase - {}' .format(pred_phase1))\n",
        "    pred_ground1 = pred_ground1[0]\n",
        "    print('pred_ground new',pred_ground1)\n",
        "    logging.debug('pred_ground - {}' .format(pred_ground1))\n",
        "    pred_twac1   = twacs_y1\n",
        "    print('pred_twac new',pred_twac1)\n",
        "    logging.debug('pred_twac - {}' .format(pred_twac1))\n",
        "    pred_pos1    = pos_y1\n",
        "    print('pred_pos new',pred_pos1)\n",
        "    logging.debug('pred_pos - {}' .format(pred_pos1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhUf7VFl7Wbd",
        "outputId": "0c185edd-aa4f-4eef-b4ee-a414c59a5ad2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape : (30600,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-4b17df31104b>:617: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  scaled_x,bias_x = np.array(normalize(rms_vec=rms_raw_vecs))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred_label prev  15110\n",
            "pred_phase prev 0\n",
            "pred_ground prev 3\n",
            "pred_twac prev 1\n",
            "pred_pos prev 1\n",
            "***************** New Model Predictions **********************\n",
            "pred_label new 15110\n",
            "pred_phase new  6\n",
            "pred_ground new 3\n",
            "pred_twac new 1\n",
            "pred_pos new 1\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EZDd_-Pc2ZKCMREeTX_uWu0CYJm5qldj",
      "authorship_tag": "ABX9TyPf71ml1Kjmgo/JvlUo06pZ"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}